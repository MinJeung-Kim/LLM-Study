{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55af41ea",
   "metadata": {},
   "source": [
    "## 자연어 처리 NLP(Natural Language Processing) | 텍스트 분석 Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456c3bf",
   "metadata": {},
   "source": [
    "- 자연어 처리 : 사람이 사용하는 언어 전반에 대해서 이해하고 처리하는 분야\n",
    "    - 음성인식, 번역, 감정분석, 질의응답, 언어생성 등 포괄적 분야\n",
    "- 테스트 분석 : 언어적 비정형 데이터에서 정보를 추출하고 분석하는 작업\n",
    "    - 텍스트 통계적 분석, 주제 분류, 텍스트 군집, 유사도 분석 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11b436",
   "metadata": {},
   "source": [
    "### 파이썬 텍스트분석 패키지 \n",
    "\n",
    "| **로고 이미지**                                                                                                 | **패키지**   | **설명**                                            | **주요 특징 및 기능**                                                   | **API 문서 URL**                                  |\n",
    "|------------------------------------------------------------------------------------------------------------|--------------|---------------------------------------------------|-----------------------------------------------------------------------|-------------------------------------------------|\n",
    "| ![nltk](https://d.pr/i/9xVzCK+)                   | **nltk**     | 가장 오래된 NLP 라이브러리 중 하나로, 다양한 자연어 처리 도구와 코퍼스 제공 | 토큰화, 품사 태깅, 어간 추출, 불용어 제거, 문법 구조 분석, 감정 분석 등에 유용 | [NLTK API Docs](https://www.nltk.org/api/nltk.html) |\n",
    "| ![gensim](https://radimrehurek.com/gensim/_static/images/gensim.png)                                       | **gensim**   | 주로 텍스트의 토픽 모델링과 문서 유사도 분석을 위한 라이브러리            | Word2Vec, FastText, LDA, 유사도 측정, 대용량 텍스트 처리에 최적화    | [Gensim API Docs](https://radimrehurek.com/gensim/) |\n",
    "| ![spacy](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/320px-SpaCy_logo.svg.png) | **spacy**    | 빠르고 효율적인 NLP 처리를 위해 개발된 라이브러리로, 산업용 프로젝트에 적합     | 빠른 토큰화, 품사 태깅, NER, 구문 분석, 벡터 표현 제공              | [SpaCy API Docs](https://spacy.io/api)             |\n",
    "| ![TextBlob](https://textblob.readthedocs.io/en/dev/_static/textblob-logo.png)                              | **TextBlob** | 간단한 NLP 작업을 위한 라이브러리로, 감정 분석과 텍스트 정제 등 지원  | 문법 교정, 감정 분석, 텍스트 번역 등과 같은 간단한 작업에 적합      | [TextBlob API Docs](https://textblob.readthedocs.io/en/dev/) |\n",
    "| ![KoNLPy](https://konlpy.org/en/latest/_static/konlpy.png)                                                 | **KoNLPy**   | 한국어 자연어 처리를 위한 라이브러리로, 여러 형태소 분석기를 제공          | Kkma, Hannanum, Komoran, Twitter, Mecab 형태소 분석기 지원            | [KoNLPy API Docs](https://konlpy.org/en/latest/)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304afb03",
   "metadata": {},
   "source": [
    "### NLTK (Natural Language Toolkit)\n",
    "- 파이썬에서 텍스트 처리 및 자연어 처리를 쉽게 다룰 수 있게 해주는 오픈 소스 라이브러리\n",
    "- NLTK는 다양한 언어 리소스와 알고리즘을 포함하고 있으며, 텍스트 마이닝, 텍스트 분석, 그리고 자연어 처리를 공부하거나 구현할 때 유용\n",
    "\n",
    "**주요 기능**\n",
    "1. **토큰화(Tokenization)**: 문장을 단어 또는 문장 단위로 나누는 작업\n",
    "    - 예를 들어, `\"I love NLP.\"`를 `['I', 'love', 'NLP', '.']`와 같이 나누는 기능을 제공한다.\n",
    "2. **품사 태깅(Part-of-Speech Tagging)**: 각 단어에 대해 해당 품사를 태깅하는 작업\n",
    "    - 예를 들어, `\"I love NLP.\"`에 대해 `[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'NNP'), ('.', '.')]`와 같이 태깅한다.\n",
    "3. **명사구 추출(Chunking)**: 문장에서 명사구와 같은 특정 구문을 추출하는 작업\n",
    "4. **어근 추출(Lemmatization) 및 어간 추출(Stemming)**: 단어의 기본 형태를 찾는 작업으로, 동사의 기본형을 찾거나 복수형을 단수형으로 변환하는 등의 작업 수행\n",
    "5. **텍스트 분류(Classification)**: Naive Bayes, MaxEnt 등의 분류 모델을 사용해 텍스트 분류 가능\n",
    "6. **코퍼스(corpus) 제공**: 영화 리뷰, 뉴스 기사 등 여러 텍스트 데이터셋을 포함하고 있어 학습과 실습에 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bf9836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!conda install nltk -y # conda의 저장소로 부터 nltk 패키지를 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e269092f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab6b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roxie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\roxie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\roxie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 리소스 다운로드\n",
    "nltk.download('punkt') # punkt tokenizer 모델 다운로드\n",
    "nltk.download('punkt_tab')  # punkt tokenizer 모델 다운로드\n",
    "nltk.download('stopwords') # 불용어 사전 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063269c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'library',\n",
       " 'for',\n",
       " 'NLP',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize # 단어 및 문장 토크나이저\n",
    "\n",
    "# 문장을 단어로 토큰화\n",
    "# - 띄어쓰기 기준(영문의 경우 띄어쓰기가 중요)\n",
    "text = \"NLTK is a powerful library for NLP!!!!!\"\n",
    "word_tokenize(text) # 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f86a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\roxie\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감성 분석\n",
    "nltk.download('vader_lexicon') # VADER 감성 분석 사전 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b392e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.189, 'pos': 0.811, 'compound': 0.8622}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer # 감성 분석 모듈\n",
    "\n",
    "# 감성 분석 객체 생성\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# SentimentIntensityAnalyzer.polarity_scores(corpus)\n",
    "# - corpus : 감성 점수를 계산할 문장(문서)\n",
    "# - return : 문서의 감성 점수를 딕셔너리로 반환\n",
    "#    - neg : 부정 감성 점수 (0 ~ 1 사이의 값)\n",
    "#    - neu : 중립 감성 점수 (0 ~ 1 사이의 값)\n",
    "#    - pos : 긍정 감성 점수 (0 ~ 1 사이의 값)\n",
    "#    - compound : 종합 감성 점수(부정, 중립, 긍정 점수를 종합하여 계산한 점수) (-1 ~ 1 사이의 값)\n",
    "\n",
    "# 문장의 감성 점수 계산\n",
    "analyser.polarity_scores(\"I love NLP. It's awesome!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524e5538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I absolutely love this!\n",
      "Score: {'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.6989}\n",
      "\n",
      "Text: This is okay, I guess.\n",
      "Score: {'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'compound': 0.2263}\n",
      "\n",
      "Text: I really hate this.\n",
      "Score: {'neg': 0.666, 'neu': 0.334, 'pos': 0.0, 'compound': -0.6115}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장에 대한 감성 분석\n",
    "# 분석할 문장들 리스트\n",
    "texts = ['I absolutely love this!', 'This is okay, I guess.', 'I really hate this.']\n",
    "\n",
    "# 각 문장의 감성 점수 계산\n",
    "for text in texts:\n",
    "    score = analyser.polarity_scores(text)\n",
    "    print(f'Text: {text}\\nScore: {score}\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea0e76e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPunctTokenizer: ['I', 'don', \"'\", 't', 'like', 'the', 'way', 'she', 'talks', 'about', 'mental', 'well', '-', 'being', '.']\n",
      "문장: I don ' t like the way she talks about mental well - being .\n",
      "WordPunctTokenizer 감성 점수: {'neg': 0.0, 'neu': 0.635, 'pos': 0.365, 'compound': 0.5574}\n"
     ]
    }
   ],
   "source": [
    "######### 토큰화 결과가 감성분석에 끼치는 영향 #########\n",
    "\n",
    "# WordPunctTokenizer : 구두점도 별도의 토큰으로 분리\n",
    "# - don't -> do, n't\n",
    "# - well-being -> well, -, being\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "corpus = \"I don't like the way she talks about mental well-being.\"\n",
    "\n",
    "# 감성 분석 객체 생성\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer() # WordPunctTokenizer 객체 생성\n",
    "tokens1 = word_punct_tokenizer.tokenize(corpus) # WordPunctTokenizer로 토큰화\n",
    "print(\"WordPunctTokenizer:\", tokens1)\n",
    "\n",
    "text1 = ' '.join(tokens1) # 토큰들을 공백으로 연결하여 문장 생성\n",
    "print(\"문장:\", text1)\n",
    "score1 = analyser.polarity_scores(text1) # 감성 점수 계산\n",
    "print(\"WordPunctTokenizer 감성 점수:\", score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f13dce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize: ['I', 'do', \"n't\", 'like', 'the', 'way', 'she', 'talks', 'about', 'mental', 'well-being', '.']\n",
      "문장: I do n't like the way she talks about mental well-being .\n",
      "word_tokenize 감성 점수: {'neg': 0.19, 'neu': 0.81, 'pos': 0.0, 'compound': -0.2755}\n"
     ]
    }
   ],
   "source": [
    "tokens2 = word_tokenize(corpus) # word_tokenize로 토큰화 \n",
    "print(\"word_tokenize:\", tokens2) # 사람이 읽는 것과 유사\n",
    "\n",
    "text2 = ' '.join(tokens2) # 토큰들을 공백으로 연결하여 문장 생성\n",
    "print(\"문장:\", text2)\n",
    "score2 = analyser.polarity_scores(text2) # 감성 점수 계산\n",
    "print(\"word_tokenize 감성 점수:\", score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "063b11cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Matrix is everywhere its all around us, here even in this room.',\n",
       " 'You can see it out your window or on your television.',\n",
       " 'You feel it when you go to work, or go to church or pay your taxes.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''The Matrix is everywhere its all around us, here even in this room.\n",
    "You can see it out your window or on your television.\n",
    "You feel it when you go to work, or go to church or pay your taxes.'''\n",
    "\n",
    "sent_tokenize(text) # 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "043f5128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 문장별 단어 토큰화\n",
    "\n",
    "# 여러 문장에 대한 단어 토큰화 함수\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text) # 문장 토큰화\n",
    "    return [word_tokenize(sentence) for sentence in sentences] # 각 문장별 단어 토큰화\n",
    "\n",
    "tokens = tokenize_text(text) # 전체 텍스트에 대한 단어 토큰화\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abe6d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]\n",
      "[('The', 'Matrix', 'is'), ('Matrix', 'is', 'everywhere'), ('is', 'everywhere', 'its'), ('everywhere', 'its', 'all'), ('its', 'all', 'around'), ('all', 'around', 'us'), ('around', 'us', ','), ('us', ',', 'here'), (',', 'here', 'even'), ('here', 'even', 'in'), ('even', 'in', 'this'), ('in', 'this', 'room'), ('this', 'room', '.')]\n"
     ]
    }
   ],
   "source": [
    "# n-gram : 연속된 n개의 단어를 하나의 토큰으로 처리하는 방법\n",
    "# - 바이그램(bigram) : 연속된 2개의 단어를 하나의 토큰으로 처리\n",
    "# - 트라이그램(trigram) : 연속된 3개의 단어를 하나의 토큰으로 처리\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "text = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "tokens = word_tokenize(text) # 단어 토큰화\n",
    "\n",
    "#  ngrams(tokens, n)\n",
    "# - tokens : 토큰 리스트\n",
    "# - n : n-그램의 n 값\n",
    "# - return : n-그램 튜플의 이터레이터\n",
    "\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(bigrams)\n",
    "\n",
    "trigrams = list(ngrams(tokens, 3)) # 트라이그램 생성\n",
    "print(trigrams) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83977135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"hadn't\", 'y', \"they've\", 'those', 'hadn', 'ain', 'do', 'itself', 'our', \"you're\", 'them', 'did', 'been', 'are', 'against', 'very', \"don't\", 'his', 'few', \"isn't\", 'were', 'hasn', 'isn', \"she'd\", 'nor', 'in', \"he'd\", 'no', 'had', \"we've\", 'having', 'for', 'he', 'when', 'shan', 'doing', 'down', 'it', 'theirs', \"weren't\", 'on', \"you've\", 'more', \"hasn't\", 'her', \"wouldn't\", \"mustn't\", \"she'll\", 'its', \"needn't\", 'can', \"shan't\", 'she', 'be', 'of', 'does', 'all', 'own', 'yours', 'just', 'here', \"we'll\", \"i've\", 'not', 'weren', 'wasn', \"you'd\", 'himself', 'being', \"they'd\", 've', 'then', \"mightn't\", 'a', 'yourselves', 'as', 'about', 'up', 'each', \"that'll\", 'him', 'some', 'from', \"he'll\", 'same', 'again', 'hers', \"it's\", 'over', 'only', 'my', 'too', \"i'd\", 'needn', 'both', 'couldn', \"couldn't\", \"doesn't\", 'whom', 't', 'that', 'their', \"it'd\", 'once', 'further', \"i'm\", 'there', 'while', 'herself', \"we're\", 'ma', 'and', 'was', 'at', 'below', 'm', \"i'll\", 'under', 'o', 'these', 'won', 'which', 'now', \"it'll\", 'll', 'after', 'you', 's', 'but', 'during', 'we', 'before', 'such', \"we'd\", \"he's\", 'didn', 'they', 'if', 'i', 'to', 'until', 'mightn', 'should', 'your', 'other', 'most', 'aren', 'has', 'mustn', \"didn't\", 'so', \"aren't\", 'themselves', 'why', \"shouldn't\", 'because', 'd', 'above', 'where', 'is', 'yourself', 'myself', 'than', 'how', 'haven', 'this', 'me', \"haven't\", 'through', 'or', 'doesn', 'an', \"wasn't\", \"you'll\", 'into', 'ourselves', 'the', 'have', \"should've\", 'by', 'who', 'don', 'off', \"they'll\", 'wouldn', \"won't\", 'out', \"she's\", 'what', 'will', 'any', 're', 'am', 'ours', 'with', 'between', 'shouldn', \"they're\"}\n"
     ]
    }
   ],
   "source": [
    "# 불용어(stopwords) 제거\n",
    "# - 문서에서 자주 등장하지만 분석에 큰 의미가 없는 단어들\n",
    "# - 예: 영어의 \"is\", \"the\", \"and\", \"in\" 등\n",
    "# - 불용어 제거는 텍스트의 핵심 의미를 파악하는 데 도움을 줌\n",
    "# - nltk.corpus.stopwords 모듈에서 다양한 언어의 불용어 리스트 제공\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids() # 사용 가능한 언어 확인(한국어 없음) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83970fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 토큰: ['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "stopwords_list = stopwords.words('english') # 영어 불용어 리스트\n",
    "\n",
    "tokens = []\n",
    "# 토큰화\n",
    "for word in word_tokenize(text): # 단어 토큰화\n",
    "    # 소문자 전환\n",
    "    word = word.lower() # 소문자 전환\n",
    "    if word not in stopwords_list: # 불용어가 아니면\n",
    "        tokens.append(word) # 토큰 리스트에 추가\n",
    "\n",
    "\n",
    "print('최종 토큰:', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdb4b9",
   "metadata": {},
   "source": [
    "### 특성 벡터화 Feature Vectorization\n",
    "1. BOW(Bag of Words): 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델이다.\n",
    "    \n",
    "   <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*S8uW62e3AbYw3gnylm4eDg.png\" width=\"500px\">\n",
    "\n",
    "2. Word Embedding: 단어를 밀집 벡터(dense vector)로 표현하는 방법으로, 단어의 의미와 관계를 보존하며 벡터로 표현한다.\n",
    "    \n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jpnKO5X0Ii8PVdQYFO2z1Q.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "| **구분**             | **Bag-of-Words (BOW)**                             | **Word Embedding**                             |\n",
    "|----------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| **개념**             | 문서를 단어의 출현 빈도로 표현                   | 단어를 실수 벡터로 표현                       |\n",
    "| **특징**             | - 단어의 순서와 의미를 고려하지 않음             | - 단어 간 의미적 유사성을 반영                |\n",
    "|                      | - 고차원, 희소 벡터 생성                         | - 밀집된 저차원 벡터 생성                     |\n",
    "| **대표 방법**        | Count Vector, TF-IDF                             | Word2Vec, GloVe, FastText                     |\n",
    "| **장점**             | - 구현이 간단하고 이해하기 쉬움                  | - 문맥 정보 반영 가능                         |\n",
    "|                      | - 단순 텍스트 데이터 분석에 유용                 | - 유사한 단어를 벡터 공간 상에서 가깝게 위치 |\n",
    "| **단점**             | - 의미적 관계와 단어의 순서 정보 없음            | - 많은 데이터와 학습 시간 필요                |\n",
    "|                      | - 고차원 희소 벡터 문제                          | - 구현이 상대적으로 복잡                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544361ab",
   "metadata": {},
   "source": [
    "### BOW > CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14d709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "text2 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe \\\n",
    "You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "\n",
    "texts = [text1, text2] # 문서 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cdbb28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "[[1 0 1 0 0 0 1 1 0 0 1 1 1 2 0 1 0 0 1 1 2 1 1 1 3 1 1 0 0 0 1 1 0 0 0 0\n",
      "  1 1 1 1 2 1 0 0 0 1 1 0 1 3 3]\n",
      " [0 4 0 1 2 1 0 0 1 1 0 0 0 0 1 0 1 1 2 0 0 0 0 0 0 0 0 2 1 1 0 0 1 1 1 2\n",
      "  0 0 4 0 1 0 1 1 1 0 0 1 0 7 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # BOW 벡터화 도구\n",
    "\n",
    "# CountVectorizer 객체 생성 : 자동으로 소문자 변환, 토큰화 수행\n",
    "vectorizer = CountVectorizer()\n",
    "# 문서 리스트를 BOW 벡터로 변환\n",
    "X = vectorizer.fit(texts) # 고유한 단어(토큰)를 추출하여 단어 사전(vocabulary) 생성\n",
    "text_vecs = X.transform(texts) \n",
    "\n",
    "print(type(text_vecs)) # 벡터화된 결과의 타입\n",
    "print(text_vecs.toarray()) # 벡터화된 결과를 배열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "894061ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'and' 'around' 'bed' 'believe' 'blue' 'can' 'church' 'deep' 'ends'\n",
      " 'even' 'everywhere' 'feel' 'go' 'goes' 'here' 'hole' 'how' 'in' 'is' 'it'\n",
      " 'its' 'matrix' 'on' 'or' 'out' 'pay' 'pill' 'rabbit' 'red' 'room' 'see'\n",
      " 'show' 'stay' 'story' 'take' 'taxes' 'television' 'the' 'this' 'to' 'us'\n",
      " 'wake' 'want' 'whatever' 'when' 'window' 'wonderland' 'work' 'you' 'your']\n",
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(X.get_feature_names_out()) # 단어 사전(피처 이름) 확인\n",
    "\n",
    "print(X.vocabulary_) # 단어 사전(피처 이름)과 인덱스 매핑 딕셔너리 확인\n",
    "# key: 단어, value: 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff4a0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>around</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bed</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blue</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>can</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>church</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ends</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>even</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>everywhere</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>feel</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>go</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>goes</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hole</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>its</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>matrix</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>on</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>or</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>out</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pay</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pill</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>red</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>room</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>see</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>show</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>stay</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>story</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>take</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>taxes</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>television</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>this</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>to</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>us</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wake</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>want</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>whatever</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>when</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>window</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>work</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>you</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>your</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  index\n",
       "0          all      0\n",
       "1          and      1\n",
       "2       around      2\n",
       "3          bed      3\n",
       "4      believe      4\n",
       "5         blue      5\n",
       "6          can      6\n",
       "7       church      7\n",
       "8         deep      8\n",
       "9         ends      9\n",
       "10        even     10\n",
       "11  everywhere     11\n",
       "12        feel     12\n",
       "13          go     13\n",
       "14        goes     14\n",
       "15        here     15\n",
       "16        hole     16\n",
       "17         how     17\n",
       "18          in     18\n",
       "19          is     19\n",
       "20          it     20\n",
       "21         its     21\n",
       "22      matrix     22\n",
       "23          on     23\n",
       "24          or     24\n",
       "25         out     25\n",
       "26         pay     26\n",
       "27        pill     27\n",
       "28      rabbit     28\n",
       "29         red     29\n",
       "30        room     30\n",
       "31         see     31\n",
       "32        show     32\n",
       "33        stay     33\n",
       "34       story     34\n",
       "35        take     35\n",
       "36       taxes     36\n",
       "37  television     37\n",
       "38         the     38\n",
       "39        this     39\n",
       "40          to     40\n",
       "41          us     41\n",
       "42        wake     42\n",
       "43        want     43\n",
       "44    whatever     44\n",
       "45        when     45\n",
       "46      window     46\n",
       "47  wonderland     47\n",
       "48        work     48\n",
       "49         you     49\n",
       "50        your     50"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# sorted() : 딕셔너리를 인덱스 기준으로 정렬\n",
    "#     - vocabulary_ : 단어 사전와 인덱스 매핑 딕셔너리\n",
    "#         - key: 단어, value: 인덱스\n",
    "#     - items() : 딕셔너리의 (키, 값) 쌍을 튜플 형태로 반환\n",
    "#     - lambda x: x[1] : 각 튜플의 두 번째 요소(인덱스)를 기준으로 정렬\n",
    "vocab = sorted(X.vocabulary_.items(), key=lambda x: x[1])  \n",
    "df_vocab = pd.DataFrame(vocab, columns=['word', 'index'])\n",
    "\n",
    "df_vocab # 빈도를 기반에서 벡터화 시킴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acd62f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>around</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blue</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>can</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>church</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ends</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>even</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>everywhere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>feel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>go</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>goes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hole</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>its</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>matrix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>or</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>out</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pill</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>room</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>see</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>show</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>stay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>story</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>take</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>taxes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>television</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>to</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>whatever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>when</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>window</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>you</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>your</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0          all      1\n",
       "1          and      4\n",
       "2       around      1\n",
       "3          bed      1\n",
       "4      believe      2\n",
       "5         blue      1\n",
       "6          can      1\n",
       "7       church      1\n",
       "8         deep      1\n",
       "9         ends      1\n",
       "10        even      1\n",
       "11  everywhere      1\n",
       "12        feel      1\n",
       "13          go      2\n",
       "14        goes      1\n",
       "15        here      1\n",
       "16        hole      1\n",
       "17         how      1\n",
       "18          in      3\n",
       "19          is      1\n",
       "20          it      2\n",
       "21         its      1\n",
       "22      matrix      1\n",
       "23          on      1\n",
       "24          or      3\n",
       "25         out      1\n",
       "26         pay      1\n",
       "27        pill      2\n",
       "28      rabbit      1\n",
       "29         red      1\n",
       "30        room      1\n",
       "31         see      1\n",
       "32        show      1\n",
       "33        stay      1\n",
       "34       story      1\n",
       "35        take      2\n",
       "36       taxes      1\n",
       "37  television      1\n",
       "38         the      5\n",
       "39        this      1\n",
       "40          to      3\n",
       "41          us      1\n",
       "42        wake      1\n",
       "43        want      1\n",
       "44    whatever      1\n",
       "45        when      1\n",
       "46      window      1\n",
       "47  wonderland      1\n",
       "48        work      1\n",
       "49         you     10\n",
       "50        your      4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 단어별 등장 횟수 구하기 ####\n",
    "\n",
    "# toarray() : 희소 행렬을 배열로 변환\n",
    "# sum(axis=0) : 각 단어의 빈도 합계 계산\n",
    "word_counts = text_vecs.toarray().sum(axis=0) # 각 단어의 빈도 합계\n",
    "\n",
    "df_vocab['count'] = df_vocab['index'].apply(lambda i: word_counts[i]) # 단어 빈도 컬럼 추가\n",
    "df_vocab = df_vocab.drop(columns=['index']) # 인덱스 컬럼 제거\n",
    "df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6882b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>church</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ends</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feel</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>goes</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hole</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>matrix</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pay</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pill</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>red</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>room</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stay</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>story</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>taxes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>television</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wake</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>want</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>window</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>work</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  index\n",
       "0          bed      0\n",
       "1      believe      1\n",
       "2         blue      2\n",
       "3       church      3\n",
       "4         deep      4\n",
       "5         ends      5\n",
       "6         feel      6\n",
       "7         goes      7\n",
       "8         hole      8\n",
       "9       matrix      9\n",
       "10         pay     10\n",
       "11        pill     11\n",
       "12      rabbit     12\n",
       "13         red     13\n",
       "14        room     14\n",
       "15        stay     15\n",
       "16       story     16\n",
       "17       taxes     17\n",
       "18  television     18\n",
       "19        wake     19\n",
       "20        want     20\n",
       "21      window     21\n",
       "22  wonderland     22\n",
       "23        work     23"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer 객체 생성 : 자동으로 소문자 변환, 토큰화 수행\n",
    "# - stop_words='english' : 영어 불용어 제거\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "texts_vecs  = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x: x[1])  \n",
    "df_vocab = pd.DataFrame(vocab, columns=['word', 'index'])\n",
    "\n",
    "df_vocab # 빈도를 기반에서 벡터화 시킴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e2ece86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bed believe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>believe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>believe red</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe want</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blue</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blue pill</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>church</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>church pay</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deep rabbit</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ends</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ends wake</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>feel</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>feel work</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>goes</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hole</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hole goes</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>matrix</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>matrix room</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pay</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pay taxes</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pill</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pill stay</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pill story</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rabbit hole</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>red</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>red pill</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>room</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  index\n",
       "0            bed      0\n",
       "1    bed believe      1\n",
       "2        believe      2\n",
       "3    believe red      3\n",
       "4   believe want      4\n",
       "5           blue      5\n",
       "6      blue pill      6\n",
       "7         church      7\n",
       "8     church pay      8\n",
       "9           deep      9\n",
       "10   deep rabbit     10\n",
       "11          ends     11\n",
       "12     ends wake     12\n",
       "13          feel     13\n",
       "14     feel work     14\n",
       "15          goes     15\n",
       "16          hole     16\n",
       "17     hole goes     17\n",
       "18        matrix     18\n",
       "19   matrix room     19\n",
       "20           pay     20\n",
       "21     pay taxes     21\n",
       "22          pill     22\n",
       "23     pill stay     23\n",
       "24    pill story     24\n",
       "25        rabbit     25\n",
       "26   rabbit hole     26\n",
       "27           red     27\n",
       "28      red pill     28\n",
       "29          room     29"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram_range : n-그램 범위 설정\n",
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words='english', \n",
    "    ngram_range=(1,2), # n-gram 범위 지정(최소값, 최대값)\n",
    "    max_features=20 # 빈도수가 상위인 n개의 데이터 추출\n",
    "    )\n",
    "texts_vecs  = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x: x[1])  \n",
    "df_vocab = pd.DataFrame(vocab, columns=['word', 'index'])\n",
    "\n",
    "df_vocab # 빈도를 기반에서 벡터화 시킴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd4cd7",
   "metadata": {},
   "source": [
    "### BOW > TfidfVectorizer  \n",
    "=> 단어의 중요성을 고려해서 가중치를 주는데,  \n",
    "=> ✨✨✨ 내 문장 안에서의 빈도는 높아야되지만, 다른 문장들과 같이 있는 전체 문장에서는 빈도가 낮아야 중요한 의미를 가진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32622138",
   "metadata": {},
   "source": [
    "- TF-IDF == Term Frequency-Inverse Document Frequency\n",
    "\n",
    "**용어** \n",
    "- $tf(t, d)$: 특정 단어 $t$가 문서 $d$에서 등장한 횟수 (Term Frequency)\n",
    "- $df(t)$: 특정 단어 $t$가 등장한 문서의 수 (Document Frequency)\n",
    "- $N$: 전체 문서의 수\n",
    "\n",
    "**TF (Term Frequency)**\n",
    "- 단어 $t$의 문서 $d$에서의 빈도를 계산하는데, 가장 일반적인 방법은 해당 단어의 단순 빈도로 정의한다.\n",
    "\n",
    "$\n",
    "tf(t, d) = \\frac{\\text{단어 } t \\text{의 문서 } d \\text{ 내 등장 횟수}}{\\text{문서 } d \\text{의 전체 단어 수}}\n",
    "$\n",
    "\n",
    "**IDF (Inverse Document Frequency)**\n",
    "- 단어가 전체 문서에서 얼마나 중요한지를 계산한다.\n",
    "- 특정 단어가 많은 문서에서 등장하면, 이 단어는 중요도가 낮아진다. 이를 반영하기 위해 아래와 같은 식을 사용한다.\n",
    "\n",
    "$\n",
    "idf(t) = \\log\\left(\\frac{N}{1 + df(t)}\\right)\n",
    "$\n",
    "\n",
    "- 여기서 $1$을 더하는 이유는, 특정 단어가 모든 문서에 등장하지 않을 경우 $df(t) = 0$이 되어, 분모가 $0$이 되는 것을 방지하기 위함이다.\n",
    "  - 예를 들어, $\\log(5/(1+1))$과 $\\log(5/(1+2))$를 계산하면, 각각 $0.3979$와 $0.2218$이 된다.\n",
    "\n",
    "**TF-IDF 계산**\n",
    "- 위의 TF와 IDF를 결합하여 TF-IDF 가중치를 계산한다.\n",
    "\n",
    "$\n",
    "\\text{tf-idf}(t, d) = tf(t, d) \\times idf(t)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94d340",
   "metadata": {},
   "source": [
    "**TfidfVectorizer의 주요 파라미터**\n",
    "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "  <tr>\n",
    "    <th>Parameter</th>\n",
    "    <th>Description</th>\n",
    "    <th>Default Value</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>max_df</b></td>\n",
    "    <td>문서의 비율 값으로서, 해당 비율 이상 나타나는 단어를 무시한다. <br> 예를 들어, max_df=0.8이면, 80% 이상의 문서에서 나타나는 단어는 제외된다.</td>\n",
    "    <td>1.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>min_df</b></td>\n",
    "    <td>문서의 비율 값 또는 정수로, 해당 비율 이하 나타나는 단어를 무시한다. <br> 예를 들어, min_df=2이면, 두 개 이하의 문서에서만 나타나는 단어는 제외된다.</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>ngram_range</b></td>\n",
    "    <td>(min_n, max_n) 형식으로, 사용할 n-gram의 범위를 정의한다. <br> 예를 들어, (1, 2)로 설정하면 unigram과 bigram을 고려한다.</td>\n",
    "    <td>(1, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>stop_words</td>\n",
    "    <td>불용어를 지정할 수 있다. \"english\"로 설정하면 영어 불용어를 사용한다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>max_features</td>\n",
    "    <td>벡터화할 때 고려할 최대 단어 수를 설정한다. 빈도순으로 상위 단어들이 선택된다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>use_idf</td>\n",
    "    <td>IDF(역문서 빈도)를 사용할지 여부를 지정한다. False로 설정하면 단순히 TF 값만 사용한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>smooth_idf</td>\n",
    "    <td>IDF 계산 시, 0으로 나누는 것을 피하기 위해 추가적인 smoothing을 수행한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sublinear_tf</td>\n",
    "    <td>TF 값에 대해 sublinear scaling (1 + log(tf))를 적용할지 지정한다.</td>\n",
    "    <td>False</td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5606034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.4472136  0.         0.         0.\n",
      "  0.         0.4472136  0.4472136  0.         0.         0.\n",
      "  0.4472136  0.        ]\n",
      " [0.21821789 0.21821789 0.43643578 0.21821789 0.21821789 0.21821789\n",
      "  0.21821789 0.         0.         0.21821789 0.21821789 0.21821789\n",
      "  0.21821789 0.         0.         0.21821789 0.21821789 0.21821789\n",
      "  0.         0.43643578]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'bed believe', 'believe', 'believe red', 'believe want',\n",
       "       'blue', 'blue pill', 'church', 'church pay', 'deep', 'deep rabbit',\n",
       "       'ends', 'ends wake', 'feel', 'feel work', 'goes', 'hole',\n",
       "       'hole goes', 'matrix', 'pill'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDF 벡터화 도구\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', \n",
    "    ngram_range=(1,2), # n-gram 범위 지정(최소값, 최대값)\n",
    "    max_features=20 # 빈도수가 상위인 n개의 데이터 추출\n",
    ")\n",
    "texts_vecs  = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "print(texts_vecs.toarray())\n",
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
