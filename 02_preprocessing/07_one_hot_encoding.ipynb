{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83092eab",
   "metadata": {},
   "source": [
    "### 원핫인코딩 (One hot encoding)\n",
    "- 범주형 데이터를 숫자로 표현하는 방법 중 하나로, 각 범주를 이진 벡터(0과 1로만 구성된 벡터)로 변환하는 기법.\n",
    "\n",
    "**주요 특징:**\n",
    "- 벡터의 길이: 전체 범주(또는 단어 집합)의 크기만큼의 길이를 가짐\n",
    "- 표현 방식: 해당 범주의 인덱스 위치만 1이고, 나머지는 모두 0\n",
    "- 상호 배타적: 각 벡터에서 정확히 하나의 요소만 1\n",
    "\n",
    "```python\n",
    "# 단어 집합: ['<OOV>', 'prince', 'little', 'pilot', 'rose']\n",
    "# 인덱스:        0         1          2          3         4\n",
    "\n",
    "# 정수 인코딩된 단어들:\n",
    "# 'little' = 2\n",
    "# 'prince' = 1\n",
    "# 'rose' = 4\n",
    "\n",
    "# 원핫 인코딩 결과:\n",
    "'little' (2) → [0, 0, 1, 0, 0]\n",
    "'prince' (1) → [0, 1, 0, 0, 0]\n",
    "'rose'   (4) → [0, 0, 0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d938e54a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c57907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어린왕자 데이터 샘플 텍스트\n",
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b583876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['little', 'prince', 'written', 'antoine', 'poetic', 'tale', 'young', 'prince', 'travels', 'home', 'planet', 'earth'], ['story', 'begins', 'pilot', 'stranded', 'sahara', 'desert', 'plane', 'crashes'], ['trying', 'fix', 'plane', 'meets', 'mysterious', 'young', 'boy', 'little', 'prince'], ['little', 'prince', 'comes', 'small', 'asteroid', 'called', 'lives', 'alone', 'rose', 'loves', 'deeply'], ['recounts', 'journey', 'pilot', 'describing', 'visits', 'several', 'planets'], ['planet', 'inhabited', 'different', 'character', 'king', 'vain', 'man', 'drunkard', 'businessman', 'geographer', 'fox'], ['encounters', 'prince', 'learns', 'valuable', 'lessons', 'love', 'responsibility', 'nature', 'adult', 'behavior'], ['earth', 'little', 'prince', 'meets', 'various', 'creatures', 'including', 'fox', 'teaches', 'relationships', 'importance', 'taming', 'means', 'building', 'ties', 'others'], ['fox', 'famous', 'line', 'become', 'responsible', 'forever', 'tamed', 'resonates', 'prince', 'feelings', 'rose'], ['ultimately', 'little', 'prince', 'realizes', 'essence', 'life', 'often', 'invisible', 'seen', 'heart'], ['sharing', 'wisdom', 'pilot', 'prepares', 'return', 'asteroid', 'beloved', 'rose'], ['story', 'concludes', 'pilot', 'reflecting', 'lessons', 'learned', 'little', 'prince', 'enduring', 'impact', 'friendship'], ['narrative', 'beautifully', 'simple', 'yet', 'profound', 'exploration', 'love', 'loss', 'importance', 'seeing', 'beyond', 'surface', 'things']]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'utils'))\n",
    "\n",
    "# 데이터 전처리 함수 임포트\n",
    "from text_preprocessing import (\n",
    "    preprocess_text_for_encoding, \n",
    ")\n",
    "\n",
    "# 1. 텍스트 전처리 (토큰화/불용어처리/정제/정규화)\n",
    "vocab, preprocessed_sentences = preprocess_text_for_encoding(raw_text)\n",
    "\n",
    "print(preprocessed_sentences)  # 전처리된 문장들 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25048193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  7  2  1  1  8  9]\n",
      " [ 0  0 10  1  4  1  1  1 11  1]\n",
      " [ 0  1  1 11 12  1  7  1  3  2]\n",
      " [ 2  1  1 13  1  1  1  5  1  1]\n",
      " [ 0  0  0  1  1  4  1  1  1  1]\n",
      " [ 1  1  1  1  1  1  1  1  1  6]\n",
      " [ 1  2  1  1 14  1  1  1  1  1]\n",
      " [ 1  6  1  1  1  1  1  1  1  1]\n",
      " [ 1  1  1  1  1  1  1  2  1  5]\n",
      " [ 1  3  2  1  1  1  1  1  1  1]\n",
      " [ 0  0  1  1  4  1  1 13  1  5]\n",
      " [ 1  4  1 14  1  3  2  1  1  1]\n",
      " [ 1  1  1  1  1  1  1  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=15,  # 단어 집합의 최대 크기 (None이면 모든 단어 사용)\n",
    "                      oov_token='<OOV>')  # OOV(Out-Of-Vocabulary) 토큰 설정\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)  # 전처리된 문장으로 단어 집합 구축\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)  # 문장들을 정수 시퀀스로 변환\n",
    "\n",
    "padded_sequences = pad_sequences(sequences , maxlen=10 ) \n",
    "print(padded_sequences)  # 패딩된 시퀀스 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a1e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 10, 15)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "one_hot_encoded = to_categorical(padded_sequences)  # 원-핫 인코딩 적용\n",
    "print(one_hot_encoded.shape)  # 원-핫 인코딩된 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd8087",
   "metadata": {},
   "source": [
    "### 한국어 전처리\n",
    "1. 토큰화(형태소 분석)\n",
    "2. 시퀀스 처리 Tokenizer\n",
    "3. 패딩처리 pad_sequences\n",
    "4. one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37d79181",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"나는 오늘 학원에 간다.\",\n",
    "    \"친구들이랑 맛있는 밥 먹을 생각에 신난다.\",\n",
    "    \"오늘 구내식당에는 뭐가 나올까?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a3689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['오늘', '학원', '간다'], ['친구', '이랑', '맛있다', '밥', '먹다', '생각', '신나다'], ['오늘', '구내식당', '에는', '뭐', '나오다']]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt # Okt 형태소 분석기 임포트\n",
    "import re # 정규표현식 임포트\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 한국어 불용어 리스트\n",
    "ko_stopwords = ['은', '는', '이', '가', '하', '고', '의', '에', '들', '을', '를', '나', '내', '우리', '도', '으로', '자', '에서', '하다']\n",
    "\n",
    "preprocess_texts = []\n",
    "for text in texts:\n",
    "    # 1. 정제 (특수문자 제거)\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    \n",
    "    # 2. 토큰화 및 불용어 처리\n",
    "    # morphs: 형태소 단위로 토큰화\n",
    "    # stem=True 옵션은 어간 추출을 의미\n",
    "    tokens = okt.morphs(text, stem=True)  # 토큰화 및 어간 추출\n",
    "    \n",
    "    # 불용어 제거\n",
    "    tokens = [word for word in tokens if word not in ko_stopwords] \n",
    "    tokens = [word for word in tokens if not re.search(r'[\\s.,;:?]', word)]  # 공백 제거\n",
    "    \n",
    "    preprocess_texts.append(tokens)\n",
    "    \n",
    "print(preprocess_texts)  # 전처리된 한국어 문장들 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8193895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4], [5, 6, 7, 8, 9, 10, 11], [2, 12, 13, 14, 15]]\n",
      "{'<OOV>': 1, '오늘': 2, '학원': 3, '간다': 4, '친구': 5, '이랑': 6, '맛있다': 7, '밥': 8, '먹다': 9, '생각': 10, '신나다': 11, '구내식당': 12, '에는': 13, '뭐': 14, '나오다': 15}\n"
     ]
    }
   ],
   "source": [
    "# 시퀀스 처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(preprocess_texts)\n",
    "sequences = tokenizer.texts_to_sequences(preprocess_texts) # 한국어 문장들을 정수 시퀀스로 변환\n",
    "print(sequences) \n",
    "print(tokenizer.word_index)  # 단어 집합 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a509de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4]\n",
      " [ 9 10 11]\n",
      " [13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# 패딩처리\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# maxlen은 문장의 길이가 유지될 정도의 값으로 설정\n",
    "padded_sequences = pad_sequences(sequences, maxlen=3)\n",
    "print(padded_sequences)  # 패딩된 시퀀스 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 16)\n",
      "[[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# 원핫 인코딩\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "one_hot_encoded = to_categorical(padded_sequences)  # 원-핫 인코딩 적용\n",
    "print(one_hot_encoded.shape)  # (3, 3, 16) : 3개의 문장, 최대 길이 3, 단어 집합 크기 16\n",
    "print(one_hot_encoded)  # 원-핫 인코딩된 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca74e9c",
   "metadata": {},
   "source": [
    "---\n",
    "## tensorflow.keras 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae66d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209</span> (836.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m209\u001b[0m (836.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">209</span> (836.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m209\u001b[0m (836.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# models : 모델 관련 클래스 및 함수 임포트\n",
    "# layers : 다양한 신경망 층 클래스 임포트\n",
    "from tensorflow.keras import models, layers \n",
    "\n",
    "# layers.Input : 입력층 정의\n",
    "#    - shape 매개변수 : 입력 데이터의 형태를 지정 (시퀀스 길이, 단어 집합 크기)\n",
    "input = layers.Input(shape=(3,16))  # 입력 시퀀스의 길이는 3 , 단어 집합 크기는 16\n",
    "\n",
    "# layers.SimpleRNN : RNN 층 정의 => 시계열 데이터 처리에 특화된 신경망 층\n",
    "#    - 8 : RNN 유닛의 크기\n",
    "#    - (input) : 이전 층의 출력이 입력으로 들어감\n",
    "x = layers.SimpleRNN(8)(input)\n",
    "\n",
    "# layers.Dense : 완전 연결층(밀집층) 정의\n",
    "#    - 1 : 출력 뉴런의 수 (이진 분류를 위해 1개)\n",
    "#    - activation='sigmoid' : 시그모이드 활성화 함수 사용 => 출력값을 0과 1 사이로 변환\n",
    "#    - (x) : 이전 층의 출력이 입력으로 들어감\n",
    "output = layers.Dense(1, activation='sigmoid')(x) \n",
    "\n",
    "# 모델 생성\n",
    "# models.Model : 모델 클래스 정의(입력층과 출력층 연결)\n",
    "# - inputs : (3, 16) \n",
    "# - outputs : (1,)\n",
    "model = models.Model(inputs=input, outputs=output)\n",
    "model.summary() # 모델 요약 출력\n",
    "# 각각의 레이어가 어떻게 연결되어 있는지, 각 층의 출력 형태와 파라미터 수를 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd666f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6667 - loss: 0.6593\n",
      "Epoch 2/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6667 - loss: 0.6518\n",
      "Epoch 3/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a337770440>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# model.compile : 모델 컴파일 => 학습 준비 단계\n",
    "#    - optimizer='adam' : Adam 옵티마이저 사용 (가중치 업데이트 방법)\n",
    "#    - loss='binary_crossentropy' : 이진 분류를 위한 손실 함수 사용\n",
    "#    - metrics=['accuracy'] : 모델 평가에 사용할 지표로 정확도 사용\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "labels = np.array([0, 1, 0])  # 예시 레이블\n",
    "\n",
    "# model.fit : 모델 학습\n",
    "#    - one_hot_encoded : 입력 데이터 (원-핫 인코딩된 시퀀스)\n",
    "#    - labels : 정답 레이블\n",
    "#    - epochs=3 : 전체 데이터셋을 3번 반복하여 학습\n",
    "model.fit(one_hot_encoded, labels, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
