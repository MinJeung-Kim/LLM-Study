{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5d4390",
   "metadata": {},
   "source": [
    "### 정수 인코딩(Integer Encoding)\n",
    "\n",
    "- 자연어 처리는 텍스트 데이터를 숫자로 변환하여 컴퓨터가 이해할 수 있도록 만드는 것이 핵심.\n",
    "- 정수 인코딩을 수행하여 텍스트 데이터에 고유한 인덱스를 부여.\n",
    "- 이러한 인코딩 과정은 전처리 과정에서 필수적이며 각 단어의 등장 빈도에 따라 인덱스를 부여하는 것이 일반적.\n",
    "- 단어 수를 5,000개로 제한하는 것은 모델 학습에 필요한 메모리와 계산 자원(리소스, 메모리 등)을 줄이기 위함.  \n",
    "    => 등장 빈도가 낮은 단어는 제외하고 상위 5,000개 단어만 선택하는 것이 일반적."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb28dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b7911",
   "metadata": {},
   "source": [
    "### 인코딩\n",
    "#### 토큰화 + 정제/정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "426084de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######### 토큰화 과정 #########\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 문장 토큰화\n",
    "# sent_tokenize() : 문장 단위로 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "# 영어 불용어 리스트\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 단어 사전(key=단어, value=빈도수)\n",
    "vocab = {}\n",
    "\n",
    "# 토큰화/정제/정규화 처리 결과\n",
    "preprocessed_sentences = []\n",
    "\n",
    "# 토큰 만큼 반복\n",
    "for sent in sentences:\n",
    "    sent = sent.lower()  # 대소문자 정규화(소문자 변환)\n",
    "    words = word_tokenize(sent)  # 단어 토큰화\n",
    "    # 불용어 제거\n",
    "    words = [word for word in words if word not in stop_words] \n",
    "    # 단어 길이가 2 이하면 제거 (필터링)\n",
    "    filtered_tokens = [word for word in words if len(word) > 2]\n",
    "    \n",
    "    # vocab 사전에 단어가 없으면 추가, 있으면 빈도수 증가\n",
    "    for word in filtered_tokens:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "    preprocessed_sentences.append(filtered_tokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "721ea28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 모듈에서 함수 임포트\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 상위 디렉토리의 utils 모듈을 임포트하기 위한 경로 추가\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'utils'))\n",
    "\n",
    "# 또는 더 간단하게:\n",
    "# sys.path.append('../utils')\n",
    "\n",
    "from text_preprocessing import (\n",
    "    preprocess_text_for_encoding,\n",
    "    create_word_to_index_mapping,\n",
    "    encode_sentences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1d7adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 단어 사전 크기: 107\n",
      "\n",
      "🔤 상위 10개 빈도수 단어:\n",
      "  prince: 9회\n",
      "  little: 6회\n",
      "  pilot: 4회\n",
      "  rose: 3회\n",
      "  fox: 3회\n",
      "  young: 2회\n",
      "  planet: 2회\n",
      "  earth: 2회\n",
      "  story: 2회\n",
      "  plane: 2회\n",
      "\n",
      "📄 전처리된 문장 수: 13\n",
      "📝 첫 번째 문장 예시: ['little', 'prince', 'written', 'antoine', 'poetic', 'tale', 'young', 'prince', 'travels', 'home']...\n"
     ]
    }
   ],
   "source": [
    "######### utils 모듈 토큰화 함수를 사용하여 전처리 수행 #########\n",
    "vocab, preprocessed_sentences = preprocess_text_for_encoding(raw_text)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"📊 단어 사전 크기:\", len(vocab))\n",
    "print(\"\\n🔤 상위 10개 빈도수 단어:\")\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, freq in sorted_vocab[:10]:\n",
    "    print(f\"  {word}: {freq}회\")\n",
    "\n",
    "print(f\"\\n📄 전처리된 문장 수: {len(preprocessed_sentences)}\")\n",
    "print(f\"📝 첫 번째 문장 예시: {preprocessed_sentences[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6b511",
   "metadata": {},
   "source": [
    "### 🔧 함수화의 장점\n",
    "\n",
    "**1. 재사용성 (Reusability)**\n",
    "- 다른 텍스트 데이터에도 동일한 전처리 적용 가능\n",
    "- 매번 같은 코드를 작성할 필요 없음\n",
    "\n",
    "**2. 매개변수 조정 (Flexibility)**\n",
    "- `min_word_length`: 최소 단어 길이 조정\n",
    "- `language`: 다른 언어의 불용어 사용 가능\n",
    "\n",
    "**3. 코드 가독성 (Readability)**\n",
    "- 복잡한 전처리 로직이 함수 안에 캡슐화\n",
    "- 메인 코드가 더 깔끔하고 이해하기 쉬움\n",
    "\n",
    "**4. 유지보수 (Maintainability)**\n",
    "- 전처리 로직 수정 시 한 곳만 변경하면 됨\n",
    "- 버그 수정이나 기능 개선이 용이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4754905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 모듈화된 함수로 전처리 시작...\n",
      "✅ 어휘 사전 크기: 102\n",
      "✅ 원본 문장 수: 13\n",
      "\n",
      "📝 첫 번째 문장 예시:\n",
      "   원본: ['little', 'prince', 'written', 'antoine', 'poetic']...\n",
      "   인코딩: [3, 2, 17, 18, 19]...\n"
     ]
    }
   ],
   "source": [
    "# 모듈화된 함수로 완전한 정수 인코딩 수행\n",
    "print(\"🔄 모듈화된 함수로 전처리 시작...\")\n",
    "\n",
    "# 1단계: 전처리\n",
    "vocab, preprocessed_sentences = preprocess_text_for_encoding(raw_text)\n",
    "\n",
    "# 2단계: 단어-인덱스 매핑 생성\n",
    "word_to_index, index_to_word = create_word_to_index_mapping(vocab, max_vocab_size=100)\n",
    "\n",
    "# 3단계: 문장들을 정수로 인코딩\n",
    "encoded_sentences = encode_sentences(preprocessed_sentences, word_to_index)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"✅ 어휘 사전 크기: {len(word_to_index)}\")\n",
    "print(f\"✅ 원본 문장 수: {len(encoded_sentences)}\")\n",
    "print(f\"\\n📝 첫 번째 문장 예시:\")\n",
    "print(f\"   원본: {preprocessed_sentences[0][:5]}...\")\n",
    "print(f\"   인코딩: {encoded_sentences[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03987be0",
   "metadata": {},
   "source": [
    "### 🚀 다른 노트북에서 사용하는 방법\n",
    "\n",
    "**1. 같은 디렉토리의 노트북 (예: `02_preprocessing/` 내)**\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from text_preprocessing import preprocess_text_for_encoding\n",
    "```\n",
    "\n",
    "**2. 다른 디렉토리의 노트북 (예: `01_text_analysis/` 내)**\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from text_preprocessing import preprocess_text_for_encoding\n",
    "```\n",
    "\n",
    "**3. 전체 패키지 임포트**\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import text_preprocessing as tp\n",
    "\n",
    "# 사용 예시\n",
    "vocab, sentences = tp.preprocess_text_for_encoding(text)\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977474c4",
   "metadata": {},
   "source": [
    "#### 빈도수 기반 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06514006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: {'little': 6, 'prince': 9, 'written': 1, 'antoine': 1, 'poetic': 1, 'tale': 1, 'young': 2, 'travels': 1, 'home': 1, 'planet': 2, 'earth': 2, 'story': 2, 'begins': 1, 'pilot': 4, 'stranded': 1, 'sahara': 1, 'desert': 1, 'plane': 2, 'crashes': 1, 'trying': 1, 'fix': 1, 'meets': 2, 'mysterious': 1, 'boy': 1, 'comes': 1, 'small': 1, 'asteroid': 2, 'called': 1, 'lives': 1, 'alone': 1, 'rose': 3, 'loves': 1, 'deeply': 1, 'recounts': 1, 'journey': 1, 'describing': 1, 'visits': 1, 'several': 1, 'planets': 1, 'inhabited': 1, 'different': 1, 'character': 1, 'king': 1, 'vain': 1, 'man': 1, 'drunkard': 1, 'businessman': 1, 'geographer': 1, 'fox': 3, 'encounters': 1, 'learns': 1, 'valuable': 1, 'lessons': 2, 'love': 2, 'responsibility': 1, 'nature': 1, 'adult': 1, 'behavior': 1, 'various': 1, 'creatures': 1, 'including': 1, 'teaches': 1, 'relationships': 1, 'importance': 2, 'taming': 1, 'means': 1, 'building': 1, 'ties': 1, 'others': 1, 'famous': 1, 'line': 1, 'become': 1, 'responsible': 1, 'forever': 1, 'tamed': 1, 'resonates': 1, 'feelings': 1, 'ultimately': 1, 'realizes': 1, 'essence': 1, 'life': 1, 'often': 1, 'invisible': 1, 'seen': 1, 'heart': 1, 'sharing': 1, 'wisdom': 1, 'prepares': 1, 'return': 1, 'beloved': 1, 'concludes': 1, 'reflecting': 1, 'learned': 1, 'enduring': 1, 'impact': 1, 'friendship': 1, 'narrative': 1, 'beautifully': 1, 'simple': 1, 'yet': 1, 'profound': 1, 'exploration': 1, 'loss': 1, 'seeing': 1, 'beyond': 1, 'surface': 1, 'things': 1}\n",
      "sorted_vocab: [('prince', 9), ('little', 6), ('pilot', 4), ('rose', 3), ('fox', 3), ('young', 2), ('planet', 2), ('earth', 2), ('story', 2), ('plane', 2), ('meets', 2), ('asteroid', 2), ('lessons', 2), ('love', 2), ('importance', 2), ('written', 1), ('antoine', 1), ('poetic', 1), ('tale', 1), ('travels', 1), ('home', 1), ('begins', 1), ('stranded', 1), ('sahara', 1), ('desert', 1), ('crashes', 1), ('trying', 1), ('fix', 1), ('mysterious', 1), ('boy', 1), ('comes', 1), ('small', 1), ('called', 1), ('lives', 1), ('alone', 1), ('loves', 1), ('deeply', 1), ('recounts', 1), ('journey', 1), ('describing', 1), ('visits', 1), ('several', 1), ('planets', 1), ('inhabited', 1), ('different', 1), ('character', 1), ('king', 1), ('vain', 1), ('man', 1), ('drunkard', 1), ('businessman', 1), ('geographer', 1), ('encounters', 1), ('learns', 1), ('valuable', 1), ('responsibility', 1), ('nature', 1), ('adult', 1), ('behavior', 1), ('various', 1), ('creatures', 1), ('including', 1), ('teaches', 1), ('relationships', 1), ('taming', 1), ('means', 1), ('building', 1), ('ties', 1), ('others', 1), ('famous', 1), ('line', 1), ('become', 1), ('responsible', 1), ('forever', 1), ('tamed', 1), ('resonates', 1), ('feelings', 1), ('ultimately', 1), ('realizes', 1), ('essence', 1), ('life', 1), ('often', 1), ('invisible', 1), ('seen', 1), ('heart', 1), ('sharing', 1), ('wisdom', 1), ('prepares', 1), ('return', 1), ('beloved', 1), ('concludes', 1), ('reflecting', 1), ('learned', 1), ('enduring', 1), ('impact', 1), ('friendship', 1), ('narrative', 1), ('beautifully', 1), ('simple', 1), ('yet', 1), ('profound', 1), ('exploration', 1), ('loss', 1), ('seeing', 1), ('beyond', 1), ('surface', 1), ('things', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(f'vocab: {vocab}')\n",
    "\n",
    "# 빈도수 기반 역순 정렬\n",
    "# sorted() => 내장 정렬 함수\n",
    "# vocab.items() => (단어, 빈도수) 튜플 리스트\n",
    "# key=lambda x: x[1] \n",
    "#     => 빈도수 기준 정렬 \n",
    "#     => lambda는 익명 함수 정의\n",
    "#     => x: x[1] => 각 튜플 x에 대해 두 번째 요소(빈도수)를 반환\n",
    "# reverse=True => 내림차순 정렬\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f'sorted_vocab: {sorted_vocab}') \n",
    "# 결과 : key: 단어, value: 빈도수 튜플 리스트 (빈도수 내림차순)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4fc797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index: {'prince': 1, 'little': 2, 'pilot': 3, 'rose': 4, 'fox': 5, 'young': 6, 'planet': 7, 'earth': 8, 'story': 9, 'plane': 10, 'meets': 11, 'asteroid': 12, 'lessons': 13, 'love': 14, 'importance': 15, 'written': 16, 'antoine': 17, 'poetic': 18, 'tale': 19, 'travels': 20, 'home': 21, 'begins': 22, 'stranded': 23, 'sahara': 24, 'desert': 25, 'crashes': 26, 'trying': 27, 'fix': 28, 'mysterious': 29, 'boy': 30, 'comes': 31, 'small': 32, 'called': 33, 'lives': 34, 'alone': 35, 'loves': 36, 'deeply': 37, 'recounts': 38, 'journey': 39, 'describing': 40, 'visits': 41, 'several': 42, 'planets': 43, 'inhabited': 44, 'different': 45, 'character': 46, 'king': 47, 'vain': 48, 'man': 49, 'drunkard': 50, 'businessman': 51, 'geographer': 52, 'encounters': 53, 'learns': 54, 'valuable': 55, 'responsibility': 56, 'nature': 57, 'adult': 58, 'behavior': 59, 'various': 60, 'creatures': 61, 'including': 62, 'teaches': 63, 'relationships': 64, 'taming': 65, 'means': 66, 'building': 67, 'ties': 68, 'others': 69, 'famous': 70, 'line': 71, 'become': 72, 'responsible': 73, 'forever': 74, 'tamed': 75, 'resonates': 76, 'feelings': 77, 'ultimately': 78, 'realizes': 79, 'essence': 80, 'life': 81, 'often': 82, 'invisible': 83, 'seen': 84, 'heart': 85, 'sharing': 86, 'wisdom': 87, 'prepares': 88, 'return': 89, 'beloved': 90, 'concludes': 91, 'reflecting': 92, 'learned': 93, 'enduring': 94, 'impact': 95, 'friendship': 96, 'narrative': 97, 'beautifully': 98, 'simple': 99, 'yet': 100, 'profound': 101, 'exploration': 102, 'loss': 103, 'seeing': 104, 'beyond': 105, 'surface': 106, 'things': 107}\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 단어 사전 생성1\n",
    "# 튜플의 리스트 형태로 된 sorted_vocab에서 단어만 추출\n",
    "# enumerate() => 인덱스와 값을 동시에 추출\n",
    "word_to_index = {word: index + 1 for index, (word, _) in enumerate(sorted_vocab)}\n",
    "print(f'word_to_index: {word_to_index}')\n",
    "# 결과 : key: 단어, value: 인덱스+1 (0은 패딩 토큰으로 예약)\n",
    "# ✨ 빈도수가 높은 단어일수록 낮은 인덱스를 가짐\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b55577a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_to_word: {1: 'prince', 2: 'little', 3: 'pilot', 4: 'rose', 5: 'fox', 6: 'young', 7: 'planet', 8: 'earth', 9: 'story', 10: 'plane', 11: 'meets', 12: 'asteroid', 13: 'lessons', 14: 'love', 15: 'importance', 16: 'written', 17: 'antoine', 18: 'poetic', 19: 'tale', 20: 'travels', 21: 'home', 22: 'begins', 23: 'stranded', 24: 'sahara', 25: 'desert', 26: 'crashes', 27: 'trying', 28: 'fix', 29: 'mysterious', 30: 'boy', 31: 'comes', 32: 'small', 33: 'called', 34: 'lives', 35: 'alone', 36: 'loves', 37: 'deeply', 38: 'recounts', 39: 'journey', 40: 'describing', 41: 'visits', 42: 'several', 43: 'planets', 44: 'inhabited', 45: 'different', 46: 'character', 47: 'king', 48: 'vain', 49: 'man', 50: 'drunkard', 51: 'businessman', 52: 'geographer', 53: 'encounters', 54: 'learns', 55: 'valuable', 56: 'responsibility', 57: 'nature', 58: 'adult', 59: 'behavior', 60: 'various', 61: 'creatures', 62: 'including', 63: 'teaches', 64: 'relationships', 65: 'taming', 66: 'means', 67: 'building', 68: 'ties', 69: 'others', 70: 'famous', 71: 'line', 72: 'become', 73: 'responsible', 74: 'forever', 75: 'tamed', 76: 'resonates', 77: 'feelings', 78: 'ultimately', 79: 'realizes', 80: 'essence', 81: 'life', 82: 'often', 83: 'invisible', 84: 'seen', 85: 'heart', 86: 'sharing', 87: 'wisdom', 88: 'prepares', 89: 'return', 90: 'beloved', 91: 'concludes', 92: 'reflecting', 93: 'learned', 94: 'enduring', 95: 'impact', 96: 'friendship', 97: 'narrative', 98: 'beautifully', 99: 'simple', 100: 'yet', 101: 'profound', 102: 'exploration', 103: 'loss', 104: 'seeing', 105: 'beyond', 106: 'surface', 107: 'things'}\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 단어 사전 생성2\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "print(f'index_to_word: {index_to_word}')\n",
    "# 결과 : key: 인덱스, value: 단어 \n",
    "# ✨ 2번 등장하는 단어는 인덱스 15번까지 이므로, vocab 사전의 크기를 15로 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7dc0766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prince': 1,\n",
       " 'little': 2,\n",
       " 'pilot': 3,\n",
       " 'rose': 4,\n",
       " 'fox': 5,\n",
       " 'young': 6,\n",
       " 'planet': 7,\n",
       " 'earth': 8,\n",
       " 'story': 9,\n",
       " 'plane': 10,\n",
       " 'meets': 11,\n",
       " 'asteroid': 12,\n",
       " 'lessons': 13,\n",
       " 'love': 14,\n",
       " 'importance': 15}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size =15\n",
    "\n",
    "# 빈도수 기반 정제\n",
    "# 단어 사전에서 vocab_size 초과하는 단어 제거 \n",
    "\n",
    "# word_to_index.items() => 기존 word_to_index 딕셔너리에서 (단어, 인덱스) 쌍을 가져옴\n",
    "# for word, index in ... => 각 (단어, 인덱스) 튜플을 순회하면서 word에는 단어, index에는 인덱스 값이 할당됨\n",
    "# if index <= vocab_size => 인덱스가 vocab_size 이하인 경우에만 조건을 만족\n",
    "# {word: index ...} => 조건을 만족하는 (단어, 인덱스) 쌍으로 새로운 딕셔너리를 생성\n",
    "word_to_idx = {word: index for word, index in word_to_index.items() if index <= vocab_size}\n",
    "\n",
    "word_to_idx\n",
    "\n",
    "# 빈도수를 기반으로 라벨링(단어 매핑)함.\n",
    "# 빈도수가 낮은 단어를 노이즈로 간주해서 제거."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc584c3",
   "metadata": {},
   "source": [
    "### oov 처리\n",
    "**OOV(Out Of Vocabulary)** : 단어사전에 정의되어 있지 않은 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8921f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_idx with OOV: {'prince': 1, 'little': 2, 'pilot': 3, 'rose': 4, 'fox': 5, 'young': 6, 'planet': 7, 'earth': 8, 'story': 9, 'plane': 10, 'meets': 11, 'asteroid': 12, 'lessons': 13, 'love': 14, 'importance': 15, 'OOV': 16}\n"
     ]
    }
   ],
   "source": [
    "word_to_idx['OOV'] = len(word_to_idx) + 1  # OOV 토큰에 새로운 인덱스 할당\n",
    "print(f'word_to_idx with OOV: {word_to_idx}')\n",
    "# 결과 : key: 단어, value: 인덱스 + OOV 토큰 추가\n",
    "# 결과 값 마지막에 'OOV': 16 => OOV 토큰에 대한 인덱스 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081df04",
   "metadata": {},
   "source": [
    "#### 수열처리 (=정수 인코딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'prince', 'written', 'antoine', 'poetic', 'tale', 'young', 'prince', 'travels', 'home', 'planet', 'earth']\n",
      "[2, 1, 16, 16, 16, 16, 6, 1, 16, 16, 7, 8]\n",
      "\n",
      "['story', 'begins', 'pilot', 'stranded', 'sahara', 'desert', 'plane', 'crashes']\n",
      "[9, 16, 3, 16, 16, 16, 10, 16]\n",
      "\n",
      "['trying', 'fix', 'plane', 'meets', 'mysterious', 'young', 'boy', 'little', 'prince']\n",
      "[16, 16, 10, 11, 16, 6, 16, 2, 1]\n",
      "\n",
      "['little', 'prince', 'comes', 'small', 'asteroid', 'called', 'lives', 'alone', 'rose', 'loves', 'deeply']\n",
      "[2, 1, 16, 16, 12, 16, 16, 16, 4, 16, 16]\n",
      "\n",
      "['recounts', 'journey', 'pilot', 'describing', 'visits', 'several', 'planets']\n",
      "[16, 16, 3, 16, 16, 16, 16]\n",
      "\n",
      "['planet', 'inhabited', 'different', 'character', 'king', 'vain', 'man', 'drunkard', 'businessman', 'geographer', 'fox']\n",
      "[7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 5]\n",
      "\n",
      "['encounters', 'prince', 'learns', 'valuable', 'lessons', 'love', 'responsibility', 'nature', 'adult', 'behavior']\n",
      "[16, 1, 16, 16, 13, 14, 16, 16, 16, 16]\n",
      "\n",
      "['earth', 'little', 'prince', 'meets', 'various', 'creatures', 'including', 'fox', 'teaches', 'relationships', 'importance', 'taming', 'means', 'building', 'ties', 'others']\n",
      "[8, 2, 1, 11, 16, 16, 16, 5, 16, 16, 15, 16, 16, 16, 16, 16]\n",
      "\n",
      "['fox', 'famous', 'line', 'become', 'responsible', 'forever', 'tamed', 'resonates', 'prince', 'feelings', 'rose']\n",
      "[5, 16, 16, 16, 16, 16, 16, 16, 1, 16, 4]\n",
      "\n",
      "['ultimately', 'little', 'prince', 'realizes', 'essence', 'life', 'often', 'invisible', 'seen', 'heart']\n",
      "[16, 2, 1, 16, 16, 16, 16, 16, 16, 16]\n",
      "\n",
      "['sharing', 'wisdom', 'pilot', 'prepares', 'return', 'asteroid', 'beloved', 'rose']\n",
      "[16, 16, 3, 16, 16, 12, 16, 4]\n",
      "\n",
      "['story', 'concludes', 'pilot', 'reflecting', 'lessons', 'learned', 'little', 'prince', 'enduring', 'impact', 'friendship']\n",
      "[9, 16, 3, 16, 13, 16, 2, 1, 16, 16, 16]\n",
      "\n",
      "['narrative', 'beautifully', 'simple', 'yet', 'profound', 'exploration', 'love', 'loss', 'importance', 'seeing', 'beyond', 'surface', 'things']\n",
      "[16, 16, 16, 16, 16, 16, 14, 16, 15, 16, 16, 16, 16]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []\n",
    "oov_idx = word_to_idx['OOV']  # OOV 토큰의 인덱스\n",
    "\n",
    "# preprocessed_sentences : 전처리된 문장 리스트\n",
    "for sent in preprocessed_sentences: \n",
    "    encoded_sentence = [word_to_idx.get(token, oov_idx) for token in sent]\n",
    "    print(sent) # 원본 문장 토큰\n",
    "    print(encoded_sentence) # 인덱스 출력 , 16은 OOV 토큰 인덱스\n",
    "    print()\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "    \n",
    "    # 결과 : 자연어 문장이 정수 시퀀스로 변환됨\n",
    "    # 단어가 word_to_idx 사전에 없으면 OOV 토큰 인덱스로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40298265",
   "metadata": {},
   "source": [
    "### kears Tokenizer\n",
    "\n",
    "- tensorflow : NLP(자연어) 처리를 위한 라이브러리\n",
    "- 딥러닝 모델 사용을 위한 라이브러리 파이토치와 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f470f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.0.0rc3)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.75.1-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorflow) (2.2.6)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp313-cp313-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp313-cp313-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl (332.0 MB)\n",
      "   ---------------------------------------- 0.0/332.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 10.0/332.0 MB 52.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 23.1/332.0 MB 58.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 28.0/332.0 MB 45.9 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 40.9/332.0 MB 49.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 50.6/332.0 MB 49.1 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 61.3/332.0 MB 49.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 71.3/332.0 MB 49.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 77.9/332.0 MB 46.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 85.5/332.0 MB 45.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 91.5/332.0 MB 44.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 97.5/332.0 MB 43.0 MB/s eta 0:00:06\n",
      "   ------------ -------------------------- 103.8/332.0 MB 41.9 MB/s eta 0:00:06\n",
      "   ------------ -------------------------- 108.8/332.0 MB 40.5 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 115.6/332.0 MB 39.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 121.6/332.0 MB 39.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 126.6/332.0 MB 38.1 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 131.6/332.0 MB 37.4 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 138.1/332.0 MB 37.1 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 144.4/332.0 MB 36.7 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 151.3/332.0 MB 36.6 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 157.0/332.0 MB 36.2 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 160.7/332.0 MB 35.3 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 163.8/332.0 MB 34.3 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 166.5/332.0 MB 33.5 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 168.6/332.0 MB 32.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 171.2/332.0 MB 31.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 172.8/332.0 MB 31.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 174.3/332.0 MB 30.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 175.9/332.0 MB 29.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 177.7/332.0 MB 28.6 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 179.8/332.0 MB 28.0 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 181.9/332.0 MB 27.5 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 184.3/332.0 MB 27.0 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 186.4/332.0 MB 26.5 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 188.5/332.0 MB 26.0 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 191.4/332.0 MB 25.6 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 193.5/332.0 MB 25.2 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 196.1/332.0 MB 24.9 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 198.2/332.0 MB 24.5 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 200.0/332.0 MB 24.1 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 201.9/332.0 MB 23.7 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 203.2/332.0 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 205.3/332.0 MB 23.0 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 207.4/332.0 MB 22.7 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 208.9/332.0 MB 22.3 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 210.8/332.0 MB 22.1 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 212.1/332.0 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 213.9/332.0 MB 21.5 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 216.0/332.0 MB 21.3 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 218.4/332.0 MB 21.0 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 221.2/332.0 MB 20.9 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 224.4/332.0 MB 20.8 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 225.7/332.0 MB 20.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 228.1/332.0 MB 20.4 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 230.4/332.0 MB 20.2 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 232.8/332.0 MB 20.1 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 233.6/332.0 MB 19.8 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 233.8/332.0 MB 19.5 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 235.9/332.0 MB 19.3 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 238.3/332.0 MB 19.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 240.9/332.0 MB 19.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 243.8/332.0 MB 19.0 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 246.2/332.0 MB 18.8 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 248.3/332.0 MB 18.7 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 251.1/332.0 MB 18.6 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 253.0/332.0 MB 18.5 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 254.0/332.0 MB 18.3 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 255.1/332.0 MB 18.1 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 256.1/332.0 MB 17.9 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 256.6/332.0 MB 17.7 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 257.7/332.0 MB 17.5 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 258.5/332.0 MB 17.3 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 259.5/332.0 MB 17.2 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 260.8/332.0 MB 17.0 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 261.9/332.0 MB 16.8 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 262.7/332.0 MB 16.7 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 263.7/332.0 MB 16.5 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 265.0/332.0 MB 16.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 266.3/332.0 MB 16.1 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 267.6/332.0 MB 15.9 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 268.7/332.0 MB 15.8 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 270.0/332.0 MB 15.6 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 270.8/332.0 MB 15.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 271.1/332.0 MB 15.3 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 271.6/332.0 MB 15.0 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 271.8/332.0 MB 14.9 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 272.4/332.0 MB 14.8 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 272.6/332.0 MB 14.7 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 272.9/332.0 MB 14.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 273.2/332.0 MB 14.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 273.4/332.0 MB 14.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 273.9/332.0 MB 13.9 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 274.5/332.0 MB 13.8 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 275.0/332.0 MB 13.7 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 276.0/332.0 MB 13.5 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 276.8/332.0 MB 13.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 277.9/332.0 MB 13.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 278.9/332.0 MB 13.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 280.0/332.0 MB 13.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 281.0/332.0 MB 12.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 282.1/332.0 MB 12.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 283.1/332.0 MB 12.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 284.4/332.0 MB 12.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 286.0/332.0 MB 12.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 287.0/332.0 MB 12.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 288.4/332.0 MB 12.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 289.7/332.0 MB 12.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 290.7/332.0 MB 12.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 291.8/332.0 MB 12.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 292.8/332.0 MB 11.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 293.9/332.0 MB 11.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 294.6/332.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 295.7/332.0 MB 11.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 296.5/332.0 MB 11.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 297.3/332.0 MB 11.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 298.1/332.0 MB 11.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 299.1/332.0 MB 11.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 300.4/332.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 301.5/332.0 MB 11.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 302.5/332.0 MB 10.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 303.6/332.0 MB 10.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 304.9/332.0 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 305.1/332.0 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 306.2/332.0 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 307.0/332.0 MB 10.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 307.5/332.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 308.3/332.0 MB 10.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 309.1/332.0 MB 10.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 310.4/332.0 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 311.4/332.0 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 312.2/332.0 MB 10.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 313.5/332.0 MB 10.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 314.6/332.0 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 315.4/332.0 MB 9.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 315.9/332.0 MB 9.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 316.7/332.0 MB 9.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 317.7/332.0 MB 9.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 318.8/332.0 MB 9.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 319.8/332.0 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 321.1/332.0 MB 9.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 322.2/332.0 MB 9.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 323.0/332.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  324.0/332.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  325.3/332.0 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  326.4/332.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  327.2/332.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  328.5/332.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  328.5/332.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  329.3/332.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  330.0/332.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.1/332.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.9/332.0 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.9/332.0 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  331.9/332.0 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 332.0/332.0 MB 8.2 MB/s  0:00:31\n",
      "Downloading grpcio-1.75.1-cp313-cp313-win_amd64.whl (4.6 MB)\n",
      "   ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.8/4.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.6/4.6 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.4/4.6 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.1/4.6 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.9/4.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.6/4.6 MB 3.8 MB/s  0:00:01\n",
      "Downloading ml_dtypes-0.5.3-cp313-cp313-win_amd64.whl (208 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.5 MB 5.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.5 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 5.7 MB/s  0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp313-cp313-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/2.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.9 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/2.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 3.8 MB/s  0:00:00\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.3/1.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 3.2 MB/s  0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/26.4 MB 4.4 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.3/26.4 MB 3.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.8/26.4 MB 3.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.9/26.4 MB 3.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.7/26.4 MB 3.7 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.5/26.4 MB 3.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 5.2/26.4 MB 3.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.3/26.4 MB 3.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.1/26.4 MB 4.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 7.9/26.4 MB 3.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 8.4/26.4 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 8.4/26.4 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.9/26.4 MB 3.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 10.0/26.4 MB 3.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.0/26.4 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.3/26.4 MB 3.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 13.6/26.4 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.2/26.4 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 17.0/26.4 MB 4.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 18.4/26.4 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.7/26.4 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.5/26.4 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.8/26.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.1/26.4 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.7/26.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 4.8 MB/s  0:00:05\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp313-cp313-win_amd64.whl (316 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt_einsum, ml_dtypes, markdown, h5py, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "\n",
      "   -- -------------------------------------  1/18 [libclang]\n",
      "   -- -------------------------------------  1/18 [libclang]\n",
      "   -- -------------------------------------  1/18 [libclang]\n",
      "   ---- -----------------------------------  2/18 [flatbuffers]\n",
      "   ---- -----------------------------------  2/18 [flatbuffers]\n",
      "   ----------- ----------------------------  5/18 [optree]\n",
      "   ----------- ----------------------------  5/18 [optree]\n",
      "   ----------- ----------------------------  5/18 [optree]\n",
      "   ----------- ----------------------------  5/18 [optree]\n",
      "   ----------- ----------------------------  5/18 [optree]\n",
      "   ------------- --------------------------  6/18 [opt_einsum]\n",
      "   ------------- --------------------------  6/18 [opt_einsum]\n",
      "   ------------- --------------------------  6/18 [opt_einsum]\n",
      "   ------------- --------------------------  6/18 [opt_einsum]\n",
      "   ------------- --------------------------  6/18 [opt_einsum]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   ----------------- ----------------------  8/18 [markdown]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   -------------------- -------------------  9/18 [h5py]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ---------------------- ----------------- 10/18 [grpcio]\n",
      "   ------------------------ --------------- 11/18 [google_pasta]\n",
      "   ------------------------ --------------- 11/18 [google_pasta]\n",
      "   ------------------------ --------------- 11/18 [google_pasta]\n",
      "   ------------------------ --------------- 11/18 [google_pasta]\n",
      "   -------------------------- ------------- 12/18 [gast]\n",
      "   ---------------------------- ----------- 13/18 [astunparse]\n",
      "   ------------------------------- -------- 14/18 [absl-py]\n",
      "   ------------------------------- -------- 14/18 [absl-py]\n",
      "   ------------------------------- -------- 14/18 [absl-py]\n",
      "   ------------------------------- -------- 14/18 [absl-py]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   --------------------------------- ------ 15/18 [tensorboard]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ----------------------------------- ---- 16/18 [keras]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ------------------------------------- -- 17/18 [tensorflow]\n",
      "   ---------------------------------------- 18/18 [tensorflow]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.1 h5py-3.15.1 keras-3.11.3 libclang-18.1.1 markdown-3.9 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_sentences: [['little', 'prince', 'written', 'antoine', 'poetic', 'tale', 'young', 'prince', 'travels', 'home', 'planet', 'earth'], ['story', 'begins', 'pilot', 'stranded', 'sahara', 'desert', 'plane', 'crashes'], ['trying', 'fix', 'plane', 'meets', 'mysterious', 'young', 'boy', 'little', 'prince'], ['little', 'prince', 'comes', 'small', 'asteroid', 'called', 'lives', 'alone', 'rose', 'loves', 'deeply'], ['recounts', 'journey', 'pilot', 'describing', 'visits', 'several', 'planets'], ['planet', 'inhabited', 'different', 'character', 'king', 'vain', 'man', 'drunkard', 'businessman', 'geographer', 'fox'], ['encounters', 'prince', 'learns', 'valuable', 'lessons', 'love', 'responsibility', 'nature', 'adult', 'behavior'], ['earth', 'little', 'prince', 'meets', 'various', 'creatures', 'including', 'fox', 'teaches', 'relationships', 'importance', 'taming', 'means', 'building', 'ties', 'others'], ['fox', 'famous', 'line', 'become', 'responsible', 'forever', 'tamed', 'resonates', 'prince', 'feelings', 'rose'], ['ultimately', 'little', 'prince', 'realizes', 'essence', 'life', 'often', 'invisible', 'seen', 'heart'], ['sharing', 'wisdom', 'pilot', 'prepares', 'return', 'asteroid', 'beloved', 'rose'], ['story', 'concludes', 'pilot', 'reflecting', 'lessons', 'learned', 'little', 'prince', 'enduring', 'impact', 'friendship'], ['narrative', 'beautifully', 'simple', 'yet', 'profound', 'exploration', 'love', 'loss', 'importance', 'seeing', 'beyond', 'surface', 'things']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'prince': 2,\n",
       " 'little': 3,\n",
       " 'pilot': 4,\n",
       " 'rose': 5,\n",
       " 'fox': 6,\n",
       " 'young': 7,\n",
       " 'planet': 8,\n",
       " 'earth': 9,\n",
       " 'story': 10,\n",
       " 'plane': 11,\n",
       " 'meets': 12,\n",
       " 'asteroid': 13,\n",
       " 'lessons': 14,\n",
       " 'love': 15,\n",
       " 'importance': 16,\n",
       " 'written': 17,\n",
       " 'antoine': 18,\n",
       " 'poetic': 19,\n",
       " 'tale': 20,\n",
       " 'travels': 21,\n",
       " 'home': 22,\n",
       " 'begins': 23,\n",
       " 'stranded': 24,\n",
       " 'sahara': 25,\n",
       " 'desert': 26,\n",
       " 'crashes': 27,\n",
       " 'trying': 28,\n",
       " 'fix': 29,\n",
       " 'mysterious': 30,\n",
       " 'boy': 31,\n",
       " 'comes': 32,\n",
       " 'small': 33,\n",
       " 'called': 34,\n",
       " 'lives': 35,\n",
       " 'alone': 36,\n",
       " 'loves': 37,\n",
       " 'deeply': 38,\n",
       " 'recounts': 39,\n",
       " 'journey': 40,\n",
       " 'describing': 41,\n",
       " 'visits': 42,\n",
       " 'several': 43,\n",
       " 'planets': 44,\n",
       " 'inhabited': 45,\n",
       " 'different': 46,\n",
       " 'character': 47,\n",
       " 'king': 48,\n",
       " 'vain': 49,\n",
       " 'man': 50,\n",
       " 'drunkard': 51,\n",
       " 'businessman': 52,\n",
       " 'geographer': 53,\n",
       " 'encounters': 54,\n",
       " 'learns': 55,\n",
       " 'valuable': 56,\n",
       " 'responsibility': 57,\n",
       " 'nature': 58,\n",
       " 'adult': 59,\n",
       " 'behavior': 60,\n",
       " 'various': 61,\n",
       " 'creatures': 62,\n",
       " 'including': 63,\n",
       " 'teaches': 64,\n",
       " 'relationships': 65,\n",
       " 'taming': 66,\n",
       " 'means': 67,\n",
       " 'building': 68,\n",
       " 'ties': 69,\n",
       " 'others': 70,\n",
       " 'famous': 71,\n",
       " 'line': 72,\n",
       " 'become': 73,\n",
       " 'responsible': 74,\n",
       " 'forever': 75,\n",
       " 'tamed': 76,\n",
       " 'resonates': 77,\n",
       " 'feelings': 78,\n",
       " 'ultimately': 79,\n",
       " 'realizes': 80,\n",
       " 'essence': 81,\n",
       " 'life': 82,\n",
       " 'often': 83,\n",
       " 'invisible': 84,\n",
       " 'seen': 85,\n",
       " 'heart': 86,\n",
       " 'sharing': 87,\n",
       " 'wisdom': 88,\n",
       " 'prepares': 89,\n",
       " 'return': 90,\n",
       " 'beloved': 91,\n",
       " 'concludes': 92,\n",
       " 'reflecting': 93,\n",
       " 'learned': 94,\n",
       " 'enduring': 95,\n",
       " 'impact': 96,\n",
       " 'friendship': 97,\n",
       " 'narrative': 98,\n",
       " 'beautifully': 99,\n",
       " 'simple': 100,\n",
       " 'yet': 101,\n",
       " 'profound': 102,\n",
       " 'exploration': 103,\n",
       " 'loss': 104,\n",
       " 'seeing': 105,\n",
       " 'beyond': 106,\n",
       " 'surface': 107,\n",
       " 'things': 108}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenizer 객체 생성\n",
    "#  - num_words : 최대 단어 수 제한 , vocab_size(상위 15개)\n",
    "#  - oov_token : OOV 토큰 지정 (vocab에 없는 단어를 처리하기 위한 토큰)\n",
    "# OOV 토큰은 훈련 데이터에 없는 단어를 처리하기 위해 사용\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "\n",
    "# fit_on_texts() : 텍스트 데이터에 맞게 토크나이저를 학습\n",
    "# preprocessed_sentences : 전처리된 문장 리스트\n",
    "print(f'preprocessed_sentences: {preprocessed_sentences}')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "tokenizer.word_index  # 단어-인덱스 매핑 출력(corpus의 모든 단어를 대상으로 생성)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fee13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<OOV>',\n",
       " 2: 'prince',\n",
       " 3: 'little',\n",
       " 4: 'pilot',\n",
       " 5: 'rose',\n",
       " 6: 'fox',\n",
       " 7: 'young',\n",
       " 8: 'planet',\n",
       " 9: 'earth',\n",
       " 10: 'story',\n",
       " 11: 'plane',\n",
       " 12: 'meets',\n",
       " 13: 'asteroid',\n",
       " 14: 'lessons',\n",
       " 15: 'love',\n",
       " 16: 'importance',\n",
       " 17: 'written',\n",
       " 18: 'antoine',\n",
       " 19: 'poetic',\n",
       " 20: 'tale',\n",
       " 21: 'travels',\n",
       " 22: 'home',\n",
       " 23: 'begins',\n",
       " 24: 'stranded',\n",
       " 25: 'sahara',\n",
       " 26: 'desert',\n",
       " 27: 'crashes',\n",
       " 28: 'trying',\n",
       " 29: 'fix',\n",
       " 30: 'mysterious',\n",
       " 31: 'boy',\n",
       " 32: 'comes',\n",
       " 33: 'small',\n",
       " 34: 'called',\n",
       " 35: 'lives',\n",
       " 36: 'alone',\n",
       " 37: 'loves',\n",
       " 38: 'deeply',\n",
       " 39: 'recounts',\n",
       " 40: 'journey',\n",
       " 41: 'describing',\n",
       " 42: 'visits',\n",
       " 43: 'several',\n",
       " 44: 'planets',\n",
       " 45: 'inhabited',\n",
       " 46: 'different',\n",
       " 47: 'character',\n",
       " 48: 'king',\n",
       " 49: 'vain',\n",
       " 50: 'man',\n",
       " 51: 'drunkard',\n",
       " 52: 'businessman',\n",
       " 53: 'geographer',\n",
       " 54: 'encounters',\n",
       " 55: 'learns',\n",
       " 56: 'valuable',\n",
       " 57: 'responsibility',\n",
       " 58: 'nature',\n",
       " 59: 'adult',\n",
       " 60: 'behavior',\n",
       " 61: 'various',\n",
       " 62: 'creatures',\n",
       " 63: 'including',\n",
       " 64: 'teaches',\n",
       " 65: 'relationships',\n",
       " 66: 'taming',\n",
       " 67: 'means',\n",
       " 68: 'building',\n",
       " 69: 'ties',\n",
       " 70: 'others',\n",
       " 71: 'famous',\n",
       " 72: 'line',\n",
       " 73: 'become',\n",
       " 74: 'responsible',\n",
       " 75: 'forever',\n",
       " 76: 'tamed',\n",
       " 77: 'resonates',\n",
       " 78: 'feelings',\n",
       " 79: 'ultimately',\n",
       " 80: 'realizes',\n",
       " 81: 'essence',\n",
       " 82: 'life',\n",
       " 83: 'often',\n",
       " 84: 'invisible',\n",
       " 85: 'seen',\n",
       " 86: 'heart',\n",
       " 87: 'sharing',\n",
       " 88: 'wisdom',\n",
       " 89: 'prepares',\n",
       " 90: 'return',\n",
       " 91: 'beloved',\n",
       " 92: 'concludes',\n",
       " 93: 'reflecting',\n",
       " 94: 'learned',\n",
       " 95: 'enduring',\n",
       " 96: 'impact',\n",
       " 97: 'friendship',\n",
       " 98: 'narrative',\n",
       " 99: 'beautifully',\n",
       " 100: 'simple',\n",
       " 101: 'yet',\n",
       " 102: 'profound',\n",
       " 103: 'exploration',\n",
       " 104: 'loss',\n",
       " 105: 'seeing',\n",
       " 106: 'beyond',\n",
       " 107: 'surface',\n",
       " 108: 'things'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word  # 인덱스-단어 매핑 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dac42161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('little', 6),\n",
       "             ('prince', 9),\n",
       "             ('written', 1),\n",
       "             ('antoine', 1),\n",
       "             ('poetic', 1),\n",
       "             ('tale', 1),\n",
       "             ('young', 2),\n",
       "             ('travels', 1),\n",
       "             ('home', 1),\n",
       "             ('planet', 2),\n",
       "             ('earth', 2),\n",
       "             ('story', 2),\n",
       "             ('begins', 1),\n",
       "             ('pilot', 4),\n",
       "             ('stranded', 1),\n",
       "             ('sahara', 1),\n",
       "             ('desert', 1),\n",
       "             ('plane', 2),\n",
       "             ('crashes', 1),\n",
       "             ('trying', 1),\n",
       "             ('fix', 1),\n",
       "             ('meets', 2),\n",
       "             ('mysterious', 1),\n",
       "             ('boy', 1),\n",
       "             ('comes', 1),\n",
       "             ('small', 1),\n",
       "             ('asteroid', 2),\n",
       "             ('called', 1),\n",
       "             ('lives', 1),\n",
       "             ('alone', 1),\n",
       "             ('rose', 3),\n",
       "             ('loves', 1),\n",
       "             ('deeply', 1),\n",
       "             ('recounts', 1),\n",
       "             ('journey', 1),\n",
       "             ('describing', 1),\n",
       "             ('visits', 1),\n",
       "             ('several', 1),\n",
       "             ('planets', 1),\n",
       "             ('inhabited', 1),\n",
       "             ('different', 1),\n",
       "             ('character', 1),\n",
       "             ('king', 1),\n",
       "             ('vain', 1),\n",
       "             ('man', 1),\n",
       "             ('drunkard', 1),\n",
       "             ('businessman', 1),\n",
       "             ('geographer', 1),\n",
       "             ('fox', 3),\n",
       "             ('encounters', 1),\n",
       "             ('learns', 1),\n",
       "             ('valuable', 1),\n",
       "             ('lessons', 2),\n",
       "             ('love', 2),\n",
       "             ('responsibility', 1),\n",
       "             ('nature', 1),\n",
       "             ('adult', 1),\n",
       "             ('behavior', 1),\n",
       "             ('various', 1),\n",
       "             ('creatures', 1),\n",
       "             ('including', 1),\n",
       "             ('teaches', 1),\n",
       "             ('relationships', 1),\n",
       "             ('importance', 2),\n",
       "             ('taming', 1),\n",
       "             ('means', 1),\n",
       "             ('building', 1),\n",
       "             ('ties', 1),\n",
       "             ('others', 1),\n",
       "             ('famous', 1),\n",
       "             ('line', 1),\n",
       "             ('become', 1),\n",
       "             ('responsible', 1),\n",
       "             ('forever', 1),\n",
       "             ('tamed', 1),\n",
       "             ('resonates', 1),\n",
       "             ('feelings', 1),\n",
       "             ('ultimately', 1),\n",
       "             ('realizes', 1),\n",
       "             ('essence', 1),\n",
       "             ('life', 1),\n",
       "             ('often', 1),\n",
       "             ('invisible', 1),\n",
       "             ('seen', 1),\n",
       "             ('heart', 1),\n",
       "             ('sharing', 1),\n",
       "             ('wisdom', 1),\n",
       "             ('prepares', 1),\n",
       "             ('return', 1),\n",
       "             ('beloved', 1),\n",
       "             ('concludes', 1),\n",
       "             ('reflecting', 1),\n",
       "             ('learned', 1),\n",
       "             ('enduring', 1),\n",
       "             ('impact', 1),\n",
       "             ('friendship', 1),\n",
       "             ('narrative', 1),\n",
       "             ('beautifully', 1),\n",
       "             ('simple', 1),\n",
       "             ('yet', 1),\n",
       "             ('profound', 1),\n",
       "             ('exploration', 1),\n",
       "             ('loss', 1),\n",
       "             ('seeing', 1),\n",
       "             ('beyond', 1),\n",
       "             ('surface', 1),\n",
       "             ('things', 1)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts  # 단어-빈도수 매핑 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcb24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2, 1, 1, 1, 1, 7, 2, 1, 1, 8, 9],\n",
       " [10, 1, 4, 1, 1, 1, 11, 1],\n",
       " [1, 1, 11, 12, 1, 7, 1, 3, 2],\n",
       " [3, 2, 1, 1, 13, 1, 1, 1, 5, 1, 1],\n",
       " [1, 1, 4, 1, 1, 1, 1],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6],\n",
       " [1, 2, 1, 1, 14, 1, 1, 1, 1, 1],\n",
       " [9, 3, 2, 12, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [6, 1, 1, 1, 1, 1, 1, 1, 2, 1, 5],\n",
       " [1, 3, 2, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 4, 1, 1, 13, 1, 5],\n",
       " [10, 1, 4, 1, 14, 1, 3, 2, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)  # 문장들을 정수 시퀀스로 변환\n",
    "sequences\n",
    "# 정수 시퀀스로 변환한다는 것은 각 단어를 해당하는 인덱스로 대체하는 것을 의미.\n",
    "\n",
    "# 결과 : \n",
    "#    인덱스 15 미만의 인덱스로 정수 인코딩을 진행하므로, 14까지의 인덱스를 확인할 수 있다.\n",
    "#    인덱스 1 = oov 토큰으로, 사전에 없는 단어는 모두 1로 인코딩된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
