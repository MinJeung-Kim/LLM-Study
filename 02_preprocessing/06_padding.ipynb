{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20696ba3",
   "metadata": {},
   "source": [
    "### Padding\n",
    "- 자연어 처리에서 각 문장(문서)의 길이는 다룰 수 있음\n",
    "- 그러나 언어모델은 고정된 길이의 데이터를 효율적으로 처리함\n",
    "- 따라서 모든 문장의 **길이를 동일하게 맞춰주는 작업**이 필요함 === 패딩\n",
    "\n",
    "**패딩 이점**\n",
    "1. 일관된 입력 형식\n",
    "2. 병렬 연산 최적화\n",
    "3. 유연한 데이터 처리\n",
    "\n",
    "- 딥러닝 모델을 사용할 때 CPU는 연산의 한계가 있어서 GPU를 사용하게 됨.\n",
    "GPU는 속도가 빠른데 병렬 처리(동시처리)를 하기 때문임.\n",
    "- 입력의 크기가 동일해야 병렬 처리가 가능함. (= 패딩 처리 = 병렬연산 최적화)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2d98c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리가 완료된 형태의 문장들(임의 설정)\n",
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
    "                          ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
    "                          ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
    "                          ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "                          ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea54f9",
   "metadata": {},
   "source": [
    "#### 직접 구현 해보기\n",
    "- 내부적으로 어떻게 동작하는지 원리에 대해 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e148f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# 패딩 토크나이저 클래스 구현\n",
    "class TokenizerForPadding:\n",
    "    def __init__(self, num_words=None, oov_token='<OOV>'):\n",
    "        self.num_words = num_words # 최대 단어 수\n",
    "        self.oov_token = oov_token # OOV 토큰\n",
    "        self.word_index = {} # 단어-인덱스 매핑\n",
    "        self.index_word = {} # 인덱스-단어 매핑\n",
    "        self.word_counts = Counter() # 단어-빈도수 매핑\n",
    "    \n",
    "    # 단어 사전 구축 메서드 구현 \n",
    "    def fit_on_texts(self, texts):\n",
    "        # 단어 빈도수 계산 \n",
    "        for sentence in texts:  \n",
    "            # self.word_counts.update(sentence)\n",
    "            self.word_counts.update(word for word in sentence if word) # 빈 문자열 제외 처리 \n",
    "        \n",
    "        # 단어를 빈도수 기준으로 vocab 구성(num_words 고려)\n",
    "        # most_common(n) : 빈도수 기준 상위 n개 단어 반환\n",
    "        vocab = [self.oov_token] + [word for word, _ in self.word_counts.most_common(self.num_words)]\n",
    "        \n",
    "        # 단어-인덱스, 인덱스-단어 매핑 생성\n",
    "        self.word_index = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "        self.index_word = {i + 1: word for i, word in enumerate(vocab)} \n",
    "\n",
    "    # 문장을 정수 시퀀스로 변환하는 메서드 구현\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.word_index.get(word, self.word_index[self.oov_token]) for word in sentence] for sentence in texts]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cfea903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 9, 6],\n",
       " [2, 4, 6],\n",
       " [10, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 7],\n",
       " [2, 5, 7],\n",
       " [2, 5, 3],\n",
       " [8, 8, 4, 3, 11, 2, 12],\n",
       " [2, 13, 4, 14]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerForPadding(num_words=15) # 단어 사전 크기 15 \n",
    "tokenizer.fit_on_texts(preprocessed_sentences)  # 단어 사전 구축\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)  # 단어-인덱스 매핑 출력\n",
    "sequences # 정수 시퀀스로 변환된 결과 출력 => 일관되지 않은 길이의 시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c60e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 함수 구현\n",
    "#   - sequences: 정수 시퀀스 리스트\n",
    "#   - maxlen: 패딩 후 시퀀스의 최대 길이 ( 데이터의 길이를 일관적으로 맞추기 위해 설정 )\n",
    "#   - padding: 'pre' 또는 'post' (앞 또는 뒤에 패딩 추가)\n",
    "#   - truncating: 'pre' 또는 'post' (앞 또는 뒤에서 시퀀스 자르기)\n",
    "#   - value: 패딩에 사용할 값 , 기본값은 0, 빈 공간을 0으로 채움\n",
    "def pad_sequences(sequences, maxlen, padding='pre', truncating='pre', value=0):\n",
    "    \n",
    "    # maxlen이 None인 경우, 시퀀스들 중 가장 긴 길이를 maxlen으로 설정\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(seq) for seq in sequences)\n",
    "        \n",
    "    padded_sequences = [] # 패딩된 시퀀스를 저장할 리스트\n",
    "    \n",
    "    # 각 시퀀스에 대해 패딩 또는 자르기 수행\n",
    "    for seq in sequences:\n",
    "        if len(seq) > maxlen:\n",
    "            # 시퀀스 자르기\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]  # 음수 인덱스를 사용한 슬라이싱 (= 앞부분을 자름)\n",
    "                # 예: [1,2,3,4,5]에서 maxlen=3이면 → [3,4,5] (앞의 1,2가 잘림)\n",
    "                \n",
    "            else:  # truncating == 'post'\n",
    "                seq = seq[:maxlen]   # 양수 인덱스를 사용한 슬라이싱 (= 뒷부분을 자름)\n",
    "                # 예: [1,2,3,4,5]에서 maxlen=3이면 → [1,2,3] (뒤의 4,5가 잘림)\n",
    "                \n",
    "        elif len(seq) < maxlen:\n",
    "            # 패딩 추가\n",
    "            pad_length = maxlen - len(seq)\n",
    "            if padding == 'pre':\n",
    "                seq = [value] * pad_length + seq  # 앞에 패딩(0) 추가\n",
    "                # 예: [1,2,3]에서 maxlen=5이면 → [0,0,1,2,3] (앞에 0이 2개 추가)\n",
    "                \n",
    "            else:  # padding == 'post'\n",
    "                seq = seq + [value] * pad_length  # 뒤에 패딩(0) 추가\n",
    "                # 예: [1,2,3]에서 maxlen=5이면 → [1,2,3,0,0] (뒤에 0이 2개 추가)\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    return torch.tensor(padded_sequences) \n",
    "    # 패딩된 시퀀스를 텐서로 반환 => 딥러닝 모델 입력으로 사용하기 위함\n",
    "    # torch.tensor() 함수는 리스트를 PyTorch 텐서로 변환하는 역할을 함\n",
    "    # 텐서는 다차원 배열로, 딥러닝 프레임워크에서 주로 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "647187ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 9, 6], [2, 4, 6], [10, 3], [3, 5, 4, 3], [4, 3], [2, 5, 7], [2, 5, 7], [2, 5, 3], [8, 8, 4, 3, 11, 2, 12], [2, 13, 4, 14]]\n",
      "tensor([[ 0,  0,  0,  0,  0,  2,  6],\n",
      "        [ 0,  0,  0,  0,  2,  9,  6],\n",
      "        [ 0,  0,  0,  0,  2,  4,  6],\n",
      "        [ 0,  0,  0,  0,  0, 10,  3],\n",
      "        [ 0,  0,  0,  3,  5,  4,  3],\n",
      "        [ 0,  0,  0,  0,  0,  4,  3],\n",
      "        [ 0,  0,  0,  0,  2,  5,  7],\n",
      "        [ 0,  0,  0,  0,  2,  5,  7],\n",
      "        [ 0,  0,  0,  0,  2,  5,  3],\n",
      "        [ 8,  8,  4,  3, 11,  2, 12],\n",
      "        [ 0,  0,  0,  2, 13,  4, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(sequences)  # 패딩 전 시퀀스 출력\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0)\n",
    "print(padded)  # 패딩된 시퀀스 출력\n",
    "\n",
    "# maxlen=None 설정 시, 가장 긴 시퀀스 길이에 맞춰 패딩이 자동으로 적용\n",
    "#  - truncating='pre'로 설정했으므로, 긴 시퀀스의 앞부분이 잘림\n",
    "#  - padding='pre'로 설정했으므로, 짧은 시퀀스의 앞부분에 0이 추가됨\n",
    "#  - value=0이므로, 패딩 값은 0으로 채워짐\n",
    "#  - 결과적으로 모든 시퀀스가 동일한 길이(7)로 맞춰짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67518b",
   "metadata": {},
   "source": [
    "### keras Tokenizer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebc82cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "[[2, 6], [2, 9, 6], [2, 4, 6], [10, 3], [3, 5, 4, 3], [4, 3], [2, 5, 7], [2, 5, 7], [2, 5, 3], [8, 8, 4, 3, 11, 2, 12], [2, 13, 4, 14]]\n",
      "[[ 0  0  0  0  0  2  6]\n",
      " [ 0  0  0  0  2  9  6]\n",
      " [ 0  0  0  0  2  4  6]\n",
      " [ 0  0  0  0  0 10  3]\n",
      " [ 0  0  0  3  5  4  3]\n",
      " [ 0  0  0  0  0  4  3]\n",
      " [ 0  0  0  0  2  5  7]\n",
      " [ 0  0  0  0  2  5  7]\n",
      " [ 0  0  0  0  2  5  3]\n",
      " [ 8  8  4  3 11  2 12]\n",
      " [ 0  0  0  2 13  4 14]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(preprocessed_sentences)  # 전처리된 문장들 출력\n",
    "\n",
    "# 토큰화 및 정수 인코딩 \n",
    "tokenizer = Tokenizer(num_words=15, oov_token='<OOV>') # 단어 사전 크기 15 , OOV 토큰 설정\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)  # 단어 사전 구축\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)  # 문장들을 정수 시퀀스로 변환\n",
    "sequences # 정수 시퀀스로 변환된 결과 출력 => 일관되지 않은 길이의 시퀀스\n",
    "\n",
    "print(sequences)  # 패딩 전 시퀀스 출력\n",
    "\n",
    "# 패딩 적용\n",
    "padded = pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0)\n",
    "print(padded)  # 패딩된 시퀀스 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64a82f",
   "metadata": {},
   "source": [
    "### [실습] 어린왕자 데이터 샘플 패딩처리\n",
    "\n",
    "1. 텍스트 전처리 (토큰화/불용어처리/정제/정규화)\n",
    "2. 정수 인코딩 Tokenizer (tensorflow.keras)\n",
    "3. 패딩 처리 pad_sequences (tensorflow.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13b7d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어린왕자 데이터 샘플 텍스트\n",
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c382611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['little', 'prince', 'written', 'antoine', 'poetic', 'tale', 'young', 'prince', 'travels', 'home', 'planet', 'earth'], ['story', 'begins', 'pilot', 'stranded', 'sahara', 'desert', 'plane', 'crashes'], ['trying', 'fix', 'plane', 'meets', 'mysterious', 'young', 'boy', 'little', 'prince'], ['little', 'prince', 'comes', 'small', 'asteroid', 'called', 'lives', 'alone', 'rose', 'loves', 'deeply'], ['recounts', 'journey', 'pilot', 'describing', 'visits', 'several', 'planets'], ['planet', 'inhabited', 'different', 'character', 'king', 'vain', 'man', 'drunkard', 'businessman', 'geographer', 'fox'], ['encounters', 'prince', 'learns', 'valuable', 'lessons', 'love', 'responsibility', 'nature', 'adult', 'behavior'], ['earth', 'little', 'prince', 'meets', 'various', 'creatures', 'including', 'fox', 'teaches', 'relationships', 'importance', 'taming', 'means', 'building', 'ties', 'others'], ['fox', 'famous', 'line', 'become', 'responsible', 'forever', 'tamed', 'resonates', 'prince', 'feelings', 'rose'], ['ultimately', 'little', 'prince', 'realizes', 'essence', 'life', 'often', 'invisible', 'seen', 'heart'], ['sharing', 'wisdom', 'pilot', 'prepares', 'return', 'asteroid', 'beloved', 'rose'], ['story', 'concludes', 'pilot', 'reflecting', 'lessons', 'learned', 'little', 'prince', 'enduring', 'impact', 'friendship'], ['narrative', 'beautifully', 'simple', 'yet', 'profound', 'exploration', 'love', 'loss', 'importance', 'seeing', 'beyond', 'surface', 'things']]\n",
      "[[3, 2, 17, 18, 19, 20, 7, 2, 21, 22, 8, 9], [10, 23, 4, 24, 25, 26, 11, 27], [28, 29, 11, 12, 30, 7, 31, 3, 2], [3, 2, 32, 33, 13, 34, 35, 36, 5, 37, 38], [39, 40, 4, 41, 42, 43, 44], [8, 45, 46, 47, 48, 49, 50, 51, 52, 53, 6], [54, 2, 55, 56, 14, 15, 57, 58, 59, 60], [9, 3, 2, 12, 61, 62, 63, 6, 64, 65, 16, 66, 67, 68, 69, 70], [6, 71, 72, 73, 74, 75, 76, 77, 2, 78, 5], [79, 3, 2, 80, 81, 82, 83, 84, 85, 86], [87, 88, 4, 89, 90, 13, 91, 5], [10, 92, 4, 93, 14, 94, 3, 2, 95, 96, 97], [98, 99, 100, 101, 102, 103, 15, 104, 16, 105, 106, 107, 108]]\n",
      "[[  3   2  17  18  19  20   7   2  21  22   8   9   0   0   0   0]\n",
      " [ 10  23   4  24  25  26  11  27   0   0   0   0   0   0   0   0]\n",
      " [ 28  29  11  12  30   7  31   3   2   0   0   0   0   0   0   0]\n",
      " [  3   2  32  33  13  34  35  36   5  37  38   0   0   0   0   0]\n",
      " [ 39  40   4  41  42  43  44   0   0   0   0   0   0   0   0   0]\n",
      " [  8  45  46  47  48  49  50  51  52  53   6   0   0   0   0   0]\n",
      " [ 54   2  55  56  14  15  57  58  59  60   0   0   0   0   0   0]\n",
      " [  9   3   2  12  61  62  63   6  64  65  16  66  67  68  69  70]\n",
      " [  6  71  72  73  74  75  76  77   2  78   5   0   0   0   0   0]\n",
      " [ 79   3   2  80  81  82  83  84  85  86   0   0   0   0   0   0]\n",
      " [ 87  88   4  89  90  13  91   5   0   0   0   0   0   0   0   0]\n",
      " [ 10  92   4  93  14  94   3   2  95  96  97   0   0   0   0   0]\n",
      " [ 98  99 100 101 102 103  15 104  16 105 106 107 108   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'utils'))\n",
    "\n",
    "# 데이터 전처리 함수 임포트\n",
    "from text_preprocessing import (\n",
    "    preprocess_text_for_encoding, \n",
    ")\n",
    "\n",
    "# 1. 텍스트 전처리 (토큰화/불용어처리/정제/정규화)\n",
    "vocab, preprocessed_sentences = preprocess_text_for_encoding(raw_text)\n",
    "\n",
    "print(preprocessed_sentences)  # 전처리된 문장들 출력\n",
    "\n",
    "# 2. 정수 인코딩 Tokenizer (tensorflow.keras)\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>') # 단어 사전 크기 1000 , OOV 토큰 설정\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)  # 단어 사전 구축\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)  # 문장들을 정수 시퀀스로 변환\n",
    "print(sequences)  # 정수 시퀀스로 변환된 결과 출력 => 일관되지 않은 길이의 시퀀스\n",
    "\n",
    "# 3. 패딩 처리 pad_sequences (tensorflow.keras)\n",
    "padded = pad_sequences(sequences, maxlen=None, padding='post', truncating='post', value=0)\n",
    "print(padded)  # 패딩된 시퀀스 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
