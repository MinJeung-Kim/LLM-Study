{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16caf9e5",
   "metadata": {},
   "source": [
    "### Subword Tokenizer\n",
    "\n",
    "| **Tokenizer 방식** | **토큰 단위**                      | **vocab size** | **미등록 단어에 대한 가정**                                                                                  |\n",
    "|---------------------|------------------------------------|----------------|-------------------------------------------------------------------------------------------------------------|\n",
    "| **사전 기반**       | 알려진 단어/형태소의 결합           | unlimited       | - 알려진 단어/형태소의 결합이라고 가정<br>- 필요한 형태소 분석 가능<br>- 사전에 등록되지 않은 단어는 UNK 처리 |\n",
    "| **sub-word**        | 알려진 글자 및 sub-word            | fixed           | - 알려진 sub-words로 분해<br>- 예: appear → app + ear<br>- 자주 등장하는 단어를 제대로 인식 가능<br>- UNK의 개수 최소화 |\n",
    "\n",
    "- wordpice 방식 : 입력되는 모든 단어에 대해서 작은 단위로 쪼갠 후 빈도가 높은 조합들간의 확률이 높은 단어들로 단어사전을 구축한다.\n",
    "- BPE(Byte Pair Encoding) 방식 : 문자의쌍을 병합하는 방식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 리뷰 데이터\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# 파일을 저장하는 함수\n",
    "def get_file(filename, origin_url):\n",
    "    # expanduser() : ~를 사용자 홈 디렉토리로 변환\n",
    "    cache_dir = os.path.expanduser('~/.torch/datasets/') # 캐시 디렉토리 경로\n",
    "    os.makedirs(cache_dir, exist_ok=True) # 디렉토리 생성 (존재하지 않을 경우)\n",
    "    filepath = os.path.join(cache_dir, filename) # 전체 파일 경로\n",
    "    \n",
    "    # exists() : 파일이나 디렉토리가 존재하는지 확인\n",
    "    if not os.path.exists(filepath): # 파일이 존재하지 않을 경우 다운로드\n",
    "        print(f'Downloading {origin_url} to {filepath}') # 다운로드 메시지 출력\n",
    "        urllib.request.urlretrieve(origin_url, filepath) # 파일 다운로드\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16058af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roxie/.torch/datasets/ratings_train.txt C:\\Users\\roxie/.torch/datasets/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "# nsmc 데이터 셋 일부 - 네이버 영화 리뷰 데이터 다운로드\n",
    "# 훈련 데이터와 테스트 데이터의 경로를 가져옴 \n",
    "ratings_train_path  = get_file('ratings_train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
    "ratings_test_path  = get_file('ratings_test.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
    "\n",
    "print(ratings_train_path, ratings_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e6758e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 형식으로 읽지만, 탭(tab)으로 구분된 파일임을 지정\n",
    "ratings_train_path_df = pd.read_csv(ratings_train_path, sep='\\t')  \n",
    "display(ratings_train_path_df.head()) \n",
    "\n",
    "ratings_test_path_df = pd.read_csv(ratings_test_path, sep='\\t')  \n",
    "display(ratings_test_path_df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0744e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train_path_df.isna().sum()  # train 데이터 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c1fe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    3\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_test_path_df.isna().sum()  # test 데이터 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f30596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149995, 3), (49997, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 제거\n",
    "# - 결측치가 하나라도 있으면 해당 행 제거\n",
    "ratings_train_path_df = ratings_train_path_df.dropna(how='any')\n",
    "ratings_test_path_df = ratings_test_path_df.dropna(how='any')  \n",
    "\n",
    "ratings_train_path_df.shape, ratings_test_path_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ce6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt 파일 생성 - 학습 데이터\n",
    "with open('naver_review.txt', 'w', encoding='utf-8') as f:\n",
    "    for doc in ratings_train_path_df['document'].values: # 'document' 열의 값들을 순회\n",
    "        f.write(doc + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c578b20d",
   "metadata": {},
   "source": [
    "### Sentence Piece Tokenizer\n",
    "- BPE와 Unigram Language Module 두가지 알고리즘을 지원한다.\n",
    "- 단어 경계에 의존하지 않으므로, 공백도 하나의 심볼로 취급한다.\n",
    "- 가장 활용도가 높음.\n",
    "- C++을 기반으로 만들어져 있기 때문에 커멘드(cmd)로 학습 시킨다.\n",
    "- 문장 전체를 하나의 문자열로 본다.(공백을 포함하여 작은 단위로 토큰화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a22753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as stp\n",
    "\n",
    "input = 'naver_review.txt' # 입력 파일\n",
    "vocab_size = 10000 # 단어 집합의 크기\n",
    "model_prefix = 'naver_review' # 모델 파일의 접두사\n",
    "cmd = f'--input={input} --model_prefix={model_prefix} --vocab_size={vocab_size}' # 학습 명령어\n",
    "\n",
    "stp.SentencePieceTrainer.Train(cmd)     # 모델 학습 후 저장\n",
    "\n",
    "# 결과 : naver_review.model, naver_review.vocab 파일이 생성됨\n",
    "# - naver_review.model : 학습된 모델 파일\n",
    "# - naver_review.vocab : 단어 집합 파일\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ddf730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "토큰화 : ['▁아', '▁더빙', '..', '▁진짜', '▁짜증나', '네요', '▁목소리']\n",
      "인덱스 : [62, 877, 5, 31, 2019, 68, 1710]\n",
      "\n",
      "원문 : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "토큰화 : ['▁흠', '...', '포스터', '보고', '▁초딩', '영화', '줄', '....', '오', '버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "인덱스 : [1634, 8, 4908, 159, 1460, 33, 264, 60, 173, 548, 410, 1224, 7396, 754, 440]\n",
      "\n",
      "원문 : 너무재밓었다그래서보는것을추천한다\n",
      "토큰화 : ['▁너무', '재', '밓', '었다', '그래서', '보', '는것을', '추천', '한다']\n",
      "인덱스 : [23, 369, 9781, 429, 3780, 143, 6266, 1945, 314]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## naver_review.model 파일로 토큰화 ########\n",
    "\n",
    "# SentencePieceProcessor() : sentencepiece 토큰화 객체 생성\n",
    "sp = stp.SentencePieceProcessor()\n",
    "\n",
    "# Load() : 학습된 모델 파일 로드\n",
    "sp.Load(f'{model_prefix}.model')\n",
    "\n",
    "# 'document' 열의 값들을 순회 (처음 3개)\n",
    "for doc in ratings_train_path_df['document'].values[:3]:  \n",
    "    print('원문 :', doc)  # 원문 출력\n",
    "    print('토큰화 :', sp.encode_as_pieces(doc))  # EncodeAsPieces:  토큰화 결과 출력\n",
    "    print('인덱스 :', sp.encode_as_ids(doc))  # EncodeAsIds: 토큰 인덱스 출력\n",
    "    print()\n",
    "    \n",
    "    # [ 결과 ]\n",
    "    # encode_as_pieces : \n",
    "    #    - 유니코드 형식으로 토큰화(공백까지 문자로 인식하여 언더바로 표시)\n",
    "    #    - 결과가 문자로 출력됨\n",
    "    # encode_as_ids : \n",
    "    #    - vocab(단어사전) 파일에 기반하여 토큰을 인덱스로 변환\n",
    "    #    - 각 토큰에 대한 고유 인덱스 부여\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "862a73d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합의 크기 확인(= vocab_size)\n",
    "sp.get_piece_size()  \n",
    "sp.GetPieceSize() \n",
    "\n",
    "# 두가지 메서드 모두 단어 집합의 크기를 반환한다.\n",
    "# get_piece_size() : 소문자 p\n",
    "# GetPieceSize() : 대문자 P\n",
    "# - 두 메서드 모두 동일한 기능을 수행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b214497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : 걸작은 몇안되고 졸작들만 넘쳐난다.\n",
      "토큰화 : ['▁걸작', '은', '▁몇', '안되고', '▁졸작', '들만', '▁넘', '쳐', '난다', '.']\n",
      "인덱스 : [1060, 18, 621, 6979, 728, 3291, 165, 705, 1003, 4]\n",
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n",
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n",
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n"
     ]
    }
   ],
   "source": [
    "# 인코딩\n",
    "text = ratings_test_path_df['document'][100]  # 테스트 데이터의 100 번째 문장\n",
    "print('원문 :', text)\n",
    "\n",
    "tokens = sp.encode_as_pieces(text) # 토큰화 (텍스트 -> subword 단위 분할)\n",
    "id_tokens = sp.encode_as_ids(text) # 토큰의 인덱스 (고유 ID 변환)\n",
    "\n",
    "print('토큰화 :', tokens)\n",
    "print('인덱스 :', id_tokens)\n",
    "# [결과]\n",
    "# 전체를 본 후 공백을 포함한다음 공백에 대한 정보들이 토큰으로 분리된 것을 확인할 수 있다.\n",
    "# 공백에 대한 정보는 유니코드 형식으로 ▁(언더바)로 표시된다.\n",
    "\n",
    "# 디코딩 (원문으로 복원)\n",
    "print(\"\".join(tokens).replace(\"▁\", \" \").strip())  # 토큰을 다시 원문으로 복원\n",
    "print(sp.decode_pieces(tokens))  # 토큰을 다시 원문으로 복원\n",
    "print(sp.decode_ids(id_tokens))  # 토큰 인덱스를 다시 원문으로 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9f7b9",
   "metadata": {},
   "source": [
    "### BertWordPieceTokenizer\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b94f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tokenizers) (0.35.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.42.1->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# 학습할 파일이 한국어로 되어 있어서 기본 설정을 변경\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    lowercase=False,  # 소문자 변환 여부\n",
    "    strip_accents=False # 악센트 제거 여부, 발음 강세문자 기호 제거 여부 (예: café -> cafe)\n",
    "    )  \n",
    "vocab_size = 10000\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[\"naver_review.txt\"],  # 학습할 파일 경로\n",
    "    # => (전처리된 파일을 넘겨주면 비지도 학습이지만 내부적으로 자기 지도 학습을함.)\n",
    "    vocab_size=vocab_size,  # 단어 집합의 크기\n",
    "    min_frequency=5,  # 최소 단어 빈도\n",
    "    show_progress=True,  # 학습 진행 상황 표시 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e68a11aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./bert_word_piece_from_naver_review-vocab.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 토크나이저 저장\n",
    "# tokenizer.save_model() : 토크나이저 모델을 지정한 경로에 저장\n",
    "tokenizer.save_model('./','bert_word_piece_from_naver_review') \n",
    "\n",
    "# [결과] : bert_word_piece_from_naver_review-vocab.txt 파일이 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c2a076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n",
      "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "토큰화 : ['걸작', '##은', '몇', '##안되고', '졸작', '##들만', '넘쳐', '##난다', '.']\n",
      "인덱스 : [2759, 1123, 445, 9507, 2589, 3799, 8334, 2430, 16]\n",
      "디코딩 : 걸작은 몇안되고 졸작들만 넘쳐난다.\n"
     ]
    }
   ],
   "source": [
    "text = ratings_test_path_df['document'][100]  # 테스트 데이터의 100번째 문장\n",
    "encoded = tokenizer.encode(text)  # 문장 인코딩\n",
    "print(text) # 원문 출력\n",
    "print(encoded) # Encoding 객체가 출력됨\n",
    "print('토큰화 :', encoded.tokens)  # 토큰화 결과 출력\n",
    "print('인덱스 :', encoded.ids)  # 토큰 인덱스 결과 출력\n",
    "\n",
    "# [결과]\n",
    "# 앞에 뭐가 붙는지 확인하는 의미로 (##) 표시가 붙음\n",
    "# 공백에 대한 정보는 토큰으로 분리되지 않음\n",
    "\n",
    "print('디코딩 :', tokenizer.decode(encoded.ids))  # 인덱스를 다시 원문으로 디코딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e48ac8",
   "metadata": {},
   "source": [
    "---\n",
    "### 전처리 연습\n",
    "\n",
    "1. 적절한 데이터셋을 찾거나 생성한다.\n",
    "2. 적절한 전처리를 진행한다.\n",
    "3. TfidfVectorizer를 이용하여 벡터화 한다. (중심 벡트를 잡는다.)\n",
    "4. Cosine Similarity를 계산하여 입력된 문자열의 긍/부정을 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bf5cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 적절한 데이터셋 생성.\n",
    "texts = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3762036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'even', 'room'], ['see', 'window', 'television'], ['feel', 'work', 'church', 'pay', 'taxes']]\n"
     ]
    }
   ],
   "source": [
    "# 2. 적절한 전처리 진행.\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'utils'))\n",
    "\n",
    "# 데이터 전처리 함수 임포트\n",
    "from text_preprocessing import (\n",
    "    preprocess_text_for_encoding, \n",
    ")\n",
    "vocab, preprocessed_sentences = preprocess_text_for_encoding(texts)\n",
    "\n",
    "print(preprocessed_sentences)  # 전처리된 문장들 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
