{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44960ac4",
   "metadata": {},
   "source": [
    "## 토큰화 (Tokenization)\n",
    "- 문장이나 단어를 더 작은 단위로 나누어 분석 가능한 단위(토큰, Token)으로 변환하는 과정\n",
    "- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 혹은 처리하는 단위로써 토큰 정의\n",
    "- 자연어 처리에서 크롤링, 데이터 수집 등으로 얻은 코퍼스 데이터는 정제되지 않은 경우가 많은데 이를 사용 용도에 맞게 토큰화, 정제, 정규화하는 과정이 필요.\n",
    "\n",
    "**토큰화 목적** \n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용 : 토큰화를 거처서 불필요한 불용어를 제거해서 의미있는 정보만 남김."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6d5dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'application', 'in', 'real-world', 'scenarios', '.']\n",
      "['NLP is fascinating.', 'It has many application in real-world scenarios.']\n",
      "[['NLP', 'is', 'fascinating', '.'], ['It', 'has', 'many', 'application', 'in', 'real-world', 'scenarios', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk # 자연어 처리 패키지 : NLTK(Natural Language Toolkit)\n",
    "# word_tokenize : 단어 토큰화 함수\n",
    "# sent_tokenize : 문장 토큰화 함수\n",
    "# nltk.download('punkt') \n",
    "#   - punkt 패키지 다운로드 (토큰화에 필요한 데이터)\n",
    "\n",
    "text = \"NLP is fascinating. It has many application in real-world scenarios.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words) # 공백을 기준으로 단어 토큰화\n",
    "\n",
    "# 문장 토큰화\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences) # 문장부호(특수문자)를 기준으로 문장 토큰화\n",
    "\n",
    "# 문장별 단어 토큰화\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(tokenized_sentences) # 각 문장을 특수문자를 기준으로 단어로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8af8a",
   "metadata": {},
   "source": [
    "## Subword Tokenizing\n",
    "- Subword : 하위 단어\n",
    "- 단어를 더 작은 단위로 토큰화 (형태소 단위)\n",
    "- vocabulary 사전에 없는 단어\n",
    "- 데이터셋에 신조어가 있을 경우 다양한 패턴을 학습함으로써 의미를 이해할 수 있다.\n",
    "- `BertTokenizer`\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함.\n",
    "    - 어휘 크기를 줄이고 다양한 언어 패턴 학습 가능.\n",
    "    - 자연어를 잘 이해함.\n",
    "    - Subword 단위로 잘 쪼개는 역할을 함.\n",
    "    \n",
    "```\n",
    "인코더 : 이해를 잘함\n",
    "디코더 : 생성을 잘함\n",
    "```\n",
    "\n",
    "```\n",
    "# 설치\n",
    "!pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c32bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', '##ha', '##pp', '##iness']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer \n",
    "\n",
    "# from_pretrained : 사전 학습된(pre-trained) BERT 모델의 토크나이저 로드\n",
    "#    - bert-base-uncased : 소문자만 사용하는 기본 BERT 모델\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "word = \"unhappiness\"\n",
    "subwords = tokenizer.tokenize(word) # 단어 토큰화 \n",
    "print(subwords) # ['un', '##ha', '##pp', '##iness'] => ## : 서브워드임을 나타내는 접두사\n",
    "\n",
    "# vocabulary 사전에 un은 있지만 happiness는 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c72ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'application',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text) \n",
    "\n",
    "# nltk.word_tokenize(text)로 단어 토큰화 한 것과 비교\n",
    "# ['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'application', 'in', 'real-world', 'scenarios', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bae6a0",
   "metadata": {},
   "source": [
    "### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6da1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']\n"
     ]
    }
   ],
   "source": [
    "###### 'unhappiness' 글자 단위로 토큰화(python 기본 함수 사용) ######\n",
    "\n",
    "# 시퀀스 자료형을 개별 문자로 분리\n",
    "char_tokens = list(word)\n",
    "print(char_tokens) # ['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5365873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'flies', 'like', 'an', 'arrow', 'fruit', 'flies', 'like', 'a', 'banana']\n"
     ]
    }
   ],
   "source": [
    "###### 정규 표현식(Regular Expression) ######\n",
    "# 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어\n",
    "# => 입력 형식에 맞게 입력이 되어 있는지 확인 할 수 있음.\n",
    "import re\n",
    "\n",
    "text = \"Time flies like an arrow; fruit flies like a banana.\" \n",
    "\n",
    "# 정규 표현식으로 단어 토큰화\n",
    "# r'' : raw string (이스케이프 문자 무시)\n",
    "# \\b : 단어 경계 문자 (단어의 시작과 끝 위치를 나타냄) => 공백, 문장 부호(특수문자) 등\n",
    "# \\w : 단어 문자 (알파벳, 숫자, _ ) \n",
    "# \\w+ : 하나 이상의 단어 문자(알파벳, 숫자, 밑줄)\n",
    "# findall : 패턴과 일치하는 모든 부분 문자열을 찾아 리스트로 반환\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(tokens)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7c6f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "\n",
    "# WordPunctTokenizer : 단어와 구두점을 모두 토큰으로 분리\n",
    "# 단어와 구두점을 개별 토큰으로 분리 (', - 등도 개별 토큰으로 분리)\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text)) \n",
    "\n",
    "print(word_tokenize(text)) # 최대한 문법적인 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16682aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/02/18', '날짜표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n",
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다', '.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/02/18', '날짜표현에', '사용될', '수', '있다', '.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다. \n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/02/18 날짜표현에 사용될 수 있다. \n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "\n",
    "# TreebankWordTokenizer : Penn Treebank 표기법을 기반으로 단어 토큰화\n",
    "# 특수문자와 숫자가 포함된 단어를 하나의 토큰으로 유지\n",
    "treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "print(treebank_word_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# [ 결과 ]\n",
    "# word_tokenize와 TreebankWordTokenizer는 거의 같은 결과를 제공하지만,\n",
    "# TreebankWordTokenizer는 특수 문자와 숫자가 포함된 단어를 하나의 토큰으로 유지하는 경향이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310caf2d",
   "metadata": {},
   "source": [
    "### KSS (Korean Sentence Splitter)\n",
    "한국어 문장 또는 한국어/영어 혼합 문장 등에 문장단위 토큰 지원\n",
    "\n",
    "`!pip install kss==5.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7aa893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n",
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pecab\\_tokenizer.py:265: RuntimeWarning: overflow encountered in scalar add\n",
      "  from_pos_data.costs[idx]\n",
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pecab\\_tokenizer.py:274: RuntimeWarning: overflow encountered in scalar add\n",
      "  least_cost += word_cost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['배경은 1920년대의 경성부이다.',\n",
       " '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.',\n",
       " \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문.\",\n",
       " '멍청이, 꼰대...가 아닐수 없다.',\n",
       " '사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kss # 한국어 문장 토큰화 패키지\n",
    "\n",
    "text = \"배경은 1920년대의 경성부이다. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문. 멍청이, 꼰대...가 아닐수 없다. 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "# split_sentences() : 문장 단위 토큰화 함수\n",
    "#     => 단순 구두점을 가지고 문장 토큰화하는 것이 아니라, 한국어 문장의 특성을 고려하여 문장 경계를 인식\n",
    "# 교착어 : 어근에 조사나 어미가 붙어 다양한 형태로 변형되는 언어\n",
    "kss.split_sentences(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0097304",
   "metadata": {},
   "source": [
    "### 품사 태깅\n",
    "\n",
    "**pos_tag**  \n",
    "\n",
    "- pos_tag는 자연어 처리(NLP)에서 단어에 품사를 태깅하는 함수로, 주로 NLTK와 같은 라이브러리에서 사용된다.\n",
    "- nltk pos_tag() 주요 품사 태깅<br>\n",
    "\n",
    "1. **NN (Noun, Singular)**  \n",
    "   단수 명사를 나타낸다. 하나의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cat\", \"book\", \"apple\"\n",
    "\n",
    "2. **NNS (Noun, Plural)**  \n",
    "   복수 명사를 나타낸다. 두 개 이상의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cats\", \"books\", \"apples\"\n",
    "\n",
    "3. **NNP (Proper Noun, Singular)**  \n",
    "   단수 고유 명사를 나타낸다. 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Alice\", \"London\", \"NASA\"\n",
    "\n",
    "4. **NNPS (Proper Noun, Plural)**  \n",
    "   복수 고유 명사를 나타낸다. 두 개 이상의 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Smiths\", \"United Nations\"\n",
    "\n",
    "5. **VB (Verb, Base Form)**  \n",
    "   동사의 원형을 나타낸다. 일반적으로 현재 시제와 함께 사용된다.  \n",
    "   예시: \"run\", \"eat\", \"play\"\n",
    "\n",
    "6. **VBD (Verb, Past Tense)**  \n",
    "   동사의 과거형을 나타낸다.  \n",
    "   예시: \"ran\", \"ate\", \"played\"\n",
    "\n",
    "7. **VBG (Verb, Gerund or Present Participle)**  \n",
    "   동명사 또는 현재 분사를 나타낸다. 일반적으로 \"-ing\" 형태이다.  \n",
    "   예시: \"running\", \"eating\", \"playing\"\n",
    "\n",
    "8. **VBN (Verb, Past Participle)**  \n",
    "   동사의 과거 분사형을 나타낸다. 주로 완료 시제와 함께 사용된다.  \n",
    "   예시: \"run\" (as in \"has run\"), \"eaten\", \"played\"\n",
    "\n",
    "9. **VBZ (Verb, 3rd Person Singular Present)**  \n",
    "   3인칭 단수 현재형 동사를 나타낸다. 주어가 3인칭 단수일 때 사용된다.  \n",
    "   예시: \"runs\", \"eats\", \"plays\"\n",
    "\n",
    "10. **JJ (Adjective)**  \n",
    "    형용사를 나타낸다. 명사를 수식하여 그 특성을 설명한다.  \n",
    "    예시: \"big\", \"blue\", \"happy\"\n",
    "\n",
    "11. **JJR (Adjective, Comparative)**  \n",
    "    비교급 형용사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"bigger\", \"bluer\", \"happier\"\n",
    "\n",
    "12. **JJS (Adjective, Superlative)**  \n",
    "    최상급 형용사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"biggest\", \"bluest\", \"happiest\"\n",
    "\n",
    "13. **RB (Adverb)**  \n",
    "    부사를 나타낸다. 동사, 형용사 또는 다른 부사를 수식한다.  \n",
    "    예시: \"quickly\", \"very\", \"well\"\n",
    "\n",
    "14. **RBR (Adverb, Comparative)**  \n",
    "    비교급 부사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"more quickly\", \"better\"\n",
    "\n",
    "15. **RBS (Adverb, Superlative)**  \n",
    "    최상급 부사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"most quickly\", \"best\"\n",
    "\n",
    "16. **IN (Preposition or Subordinating Conjunction)**  \n",
    "    전치사 또는 종속 접속사를 나타낸다. 명사와의 관계를 나타내거나 종속절을 시작한다.  \n",
    "    예시: \"in\", \"on\", \"because\"\n",
    "\n",
    "17. **DT (Determiner)**  \n",
    "    한정사를 나타낸다. 명사의 수와 상태를 정의한다.  \n",
    "    예시: \"the\", \"a\", \"some\"\n",
    "\n",
    "18. **PRP (Personal Pronoun)**  \n",
    "    인칭 대명사를 나타낸다. 사람, 사물 등을 대체할 때 사용된다.  \n",
    "    예시: \"I\", \"you\", \"he\", \"they\"\n",
    "\n",
    "19. **PRP$ (Possessive Pronoun)**  \n",
    "    소유 대명사를 나타낸다. 소유 관계를 나타낸다.  \n",
    "    예시: \"my\", \"your\", \"his\", \"their\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828980fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\roxie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger') # 영문 전용 품사 태깅(POS tagging) 모델 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e935bb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Time flies like an arrow.\" \n",
    "tokens = word_tokenize(text)\n",
    "post_tag = pos_tag(tokens)\n",
    "post_tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee57eda",
   "metadata": {},
   "source": [
    "**spacy 주요 품사 태깅**\n",
    "\n",
    "| 태그 | 설명                 | 예시                 |\n",
    "|------|----------------------|----------------------|\n",
    "| ADJ  | 형용사               | big, nice           |\n",
    "| ADP  | 전치사               | in, to, on          |\n",
    "| ADV  | 부사                 | very, well          |\n",
    "| AUX  | 조동사               | is, have (조동사로 사용될 때) |\n",
    "| CONJ | 접속사               | and, or             |\n",
    "| DET  | 한정사/관사          | the, a              |\n",
    "| INTJ | 감탄사               | oh, wow             |\n",
    "| NOUN | 명사                 | dog, table          |\n",
    "| NUM  | 숫자                 | one, two, 3         |\n",
    "| PART | 소사                 | 'to' (to fly에서), not |\n",
    "| PRON | 대명사               | he, she, it         |\n",
    "| PROPN| 고유명사             | John, France        |\n",
    "| PUNCT| 구두점               | ., !, ?             |\n",
    "| SCONJ| 종속 접속사          | because, if         |\n",
    "| SYM  | 기호                 | $, %, @             |\n",
    "| VERB | 동사                 | run, eat            |\n",
    "| X    | 알 수 없는 품사       | 외국어 단어, 잘못된 형식 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86dfcfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.11.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0rc3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f808cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# spacy : 고성능 자연어 처리 패키지\n",
    "import spacy\n",
    "\n",
    "# spacy.cli.download() : 사전 학습된(pre-trained) SpaCy 모델 다운로드\n",
    "# \"en_core_web_sm\" : 영어 소형 모델\n",
    "spacy.cli.download(\"en_core_web_sm\")  \n",
    "\n",
    "# spacy.load() : 사전 학습된(pre-trained) SpaCy 모델 로드\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a320cd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time flies like an arrow.\n",
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_nlp(text) # 문장 토큰화 및 품사 태깅\n",
    "print(tokens) # 문장이 그대로 출력되지만 내부적으로 토큰으로 나눠진 형태.\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, \":\", token.pos_) # 토큰화된 단어와 해당 단어의 품사 태그"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946c48f",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능\n",
    "\n",
    "#### Error \n",
    "```bash\n",
    "JVMNotFoundException: No JVM shared library file (jvm.dll) found.\n",
    "```\n",
    "- Okt는 자바 기반 라이브러리이므로, 자바 가상 머신(JVM)이 필요\n",
    "=> 자바개발 키트(JDK) 설치 - temurin jdk\n",
    "\n",
    "**다운로드**  \n",
    "https://adoptium.net/temurin/releases/?version=17&os=any&arch=any  \n",
    "Windows x64\n",
    "\n",
    "**환경 변수 설정**  \n",
    "제어판 -> 시스템 환경 변수 편집 -> 환경 변수(N)... -> 시스템 변수(S) -> 새로 만들기 -> 변수 이름 : JAVA_HOME, 변수 값 : D:\\PlayDataStudy\\dev\\jdk-17.0.16+8\n",
    "\n",
    "**확인 - cmd**  \n",
    "\n",
    "```bash\n",
    "$ java -version\n",
    "\n",
    "# 17.0.16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d197cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from konlpy) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\roxie\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from konlpy) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\roxie\\appdata\\roaming\\python\\python313\\site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f17693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '점심', '은', '뭘', '먹어', '볼까', '.', '맛있는', '게', '뭐', '지', '?']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = \"오늘 점심은 뭘 먹어볼까. 맛있는 게 뭐지?\"\n",
    "okt = Okt() # Okt 형태소 분석기 객체 생성 \n",
    "\n",
    "# morphs() : 형태소 단위로 토큰화\n",
    "morphs = okt.morphs(text) \n",
    "print(morphs)\n",
    "\n",
    "### Error Example ###\n",
    "# JVMNotFoundException: No JVM shared library file (jvm.dll) found.\n",
    "# Okt는 자바 기반 라이브러리이므로, 자바 가상 머신(JVM)이 필요\n",
    "# => 자바개발 키트(JDK) 설치 - temurin jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91311ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('점심', 'Noun'), ('은', 'Josa'), ('뭘', 'Noun'), ('먹어', 'Verb'), ('볼까', 'Verb'), ('.', 'Punctuation'), ('맛있는', 'Adjective'), ('게', 'Noun'), ('뭐', 'Noun'), ('지', 'Josa'), ('?', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = okt.pos(text) # 형태소 단위로 토큰화 및 품사 태깅\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da9dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '점심', '뭘', '게', '뭐']\n"
     ]
    }
   ],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "print(nouns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
