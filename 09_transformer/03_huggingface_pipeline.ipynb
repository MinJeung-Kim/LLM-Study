{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212e1d30",
   "metadata": {},
   "source": [
    "# HugginFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f198af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q kobert-transformers\n",
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e00178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f97b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1030a",
   "metadata": {},
   "source": [
    "### NLP Tasks\n",
    "\n",
    "주어진 Task에서 사전 훈련된 모델을 사용하는 가장 간단한 방법은 `pipeline`을 사용하는 것이다.\n",
    "\n",
    "- **기계 번역(Translation)**: 텍스트를 다른 언어로 번역한다.  \n",
    "- **감정 분석(Text Classification)**: 텍스트가 긍정적인지 부정적인지 분류할 수 있다.  \n",
    "- **텍스트 생성(Text Generation)**: 프롬프트를 입력하면 모델이 후속 텍스트를 생성한다.  \n",
    "- **이름 개체 인식(NER)**: 입력 문장의 각 단어가 어떤 개체(예: 사람, 장소 등)를 나타내는지 식별할 수 있다.  \n",
    "- **질문 답변(Question Answering)**: 컨텍스트와 질문을 입력하면 모델이 컨텍스트에서 적절한 답변을 추출한다.  \n",
    "- **마스킹된 텍스트 채우기(Fill-Mask)**: 마스킹된 단어가 포함된 텍스트(`[MASK]`로 대체됨)를 입력하면 공백을 채울 단어를 예측한다.  \n",
    "- **요약(Summarization)**: 긴 텍스트의 요약을 생성한다.  \n",
    "- **특징 추출(Feature Extraction)**: 텍스트의 텐서 표현을 반환하여 특성을 추출한다.  \n",
    "- **Zero-Shot 분류(Zero-Shot Classification)**: 레이블이 없는 데이터에 대해 사전 정의된 레이블에 맞는 분류를 수행한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a9cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63baf4d9",
   "metadata": {},
   "source": [
    "##### 기계번역\n",
    "\n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-ko-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c4939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.TranslationPipeline at 0x124b4cad550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = pipeline('translation', model='Helsinki-NLP/opus-mt-ko-en')\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007589b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'I want to go home now.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation(\"저 지금 집에 가고 싶어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f2596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"It's snowy. It's snowy.\"},\n",
       " {'translation_text': '19th stage.'},\n",
       " {'translation_text': \"It doesn't make any sense to me.\"},\n",
       " {'translation_text': 'Learning nlp is fun.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation([\n",
    "    '눈이 내려서 눈이 시려요.',\n",
    "    '19기 화이팅',\n",
    "    '말을 보니 말이 안나와요.',\n",
    "    'nlp공부는 재미있다.'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3654f7",
   "metadata": {},
   "source": [
    "##### 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d49e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_clf = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b061003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998770952224731}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"I'm very happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c50d0d",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464da34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_sentiment_clf = pipeline('sentiment-analysis', \n",
    "                            model='sangrimlee/bert-base-multilingual-cased-nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f56998c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9876468181610107},\n",
       " {'label': 'positive', 'score': 0.970969557762146},\n",
       " {'label': 'negative', 'score': 0.9228838086128235},\n",
       " {'label': 'positive', 'score': 0.7536783814430237}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_sentiment_clf([\n",
    "    '이 프로젝트는 잘 진행되고 있어!',\n",
    "    '열심히 한 만큼 결과도 좋게 나올 거야!',\n",
    "    '나한테 관심도 없는 것 같아 ㅠㅠ',\n",
    "    '컴퓨터가 자동으로 업데이트되었어.'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ec60a",
   "metadata": {},
   "source": [
    "##### Zero Shot Classification\n",
    "\n",
    "- shot == 예시\n",
    "- 예시 없이 추론 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e78508c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42369a80f2134a4fb1cf1a22962a65c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  78%|#######7  | 1.27G/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbabfb9c2645403cabb5faeecd8156c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72c263394f44ad79a0d4ffb035679fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6880cd44f6d48eda0c935ec9e6c172d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92154555d9a24a85912fc91fe8bbd116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "zero_shot_clf = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c00473d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the transformers library.',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9053581953048706, 0.07259626686573029, 0.022045595571398735]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"This is a course about the transformers library.\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb44e8e",
   "metadata": {},
   "source": [
    "##### 한국어 Zero Shot\n",
    "\n",
    "https://huggingface.co/joeddav/xlm-roberta-large-xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf949a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398820bd4a8f4194b844f7c098524810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--joeddav--xlm-roberta-large-xnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb03bffe833d4912ba7c12b65d51d8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f2f80364cc4ef98569ecf2499bbe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5530aa684fc842feb9bd033c60fa68ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c9184fafbe4d5e8bfe0eb5805e72ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_zero_shot_clf = pipeline('zero-shot-classification', model='joeddav/xlm-roberta-large-xnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c8b3d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025년은 어떤 운동을 하면 좋을까?',\n",
       " 'labels': ['건강', '정치', '경제'],\n",
       " 'scores': [0.9276527762413025, 0.05568736046552658, 0.01665988750755787]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"2025년은 어떤 운동을 하면 좋을까?\"\n",
    "candidate_labels = [\"정치\", \"경제\", \"건강\"]\n",
    "hypothesis_template = '이 텍스트는 {}에 관한 내용입니다.'\n",
    "\n",
    "ko_zero_shot_clf(sequence_to_classify, candidate_labels=candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdc4fa",
   "metadata": {},
   "source": [
    "##### 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b6aa6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1728a77cec0d4cde9edfc50cc38131b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de36517c760841c5a56b947bbb3534f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f324e91b5888457cb9bc968ac09adb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3350d33f347a480a86627b7ad70b6910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33858a6322ce47feb879d5a681a1d117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59fc5a489924f0294543fd8f6f385ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d26c4dff5447b585c2320246d864a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline('text-generation', model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f58cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use CSS, how to build a website, and how to use the JavaScript library. If you want to learn more, read our course in CSS and how to use Javascript, learn more at CSS.com. It is free.\\n\\n\\n\\nWhat to use in your first project?\\nIf you are ready to learn more, you can visit our website if you are familiar with the basics of Node.js. You can learn more about these basics and explore what to use in your project.'},\n",
       " {'generated_text': 'In this course, we will teach you how to write a simple Python application.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc6ce9",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e31e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb1c08c034748489e97450d4a115996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--skt--ko-gpt-trinity-1.2B-v0.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216fb4ab1c174b259ad17b90d68848bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf104ff356484f5d88f663c89a82e513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1ce9ec4266487295198b5fdf3ad524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ba007f40214d23853fd841e476dc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_text_generator = pipeline('text-generation', model='skt/ko-gpt-trinity-1.2B-v0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08a73dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '오늘 점심에는 뭐 먹었어\\n 답변:그럼요. 혹시 식사 전이시면 저에게 메뉴 추천해줘\"라고 해보세요. 맛있는 메뉴 추천드릴게요.\"'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_text_generator('오늘 점심에는')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f73119",
   "metadata": {},
   "source": [
    "##### Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05626380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270d09ba04bf4c61b6ed4561c8f5b51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a495a96b54c3409eb812af3d37245514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580f84baab104b7c9445b054dddad2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1846aedfa014e45854be848204edb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5714aadef07f4b64a5285b6a4db68032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d292b4b476dc4d939246dde92218e41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9be944d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09073042869567871,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': 'This course will teach you all about business model.'},\n",
       " {'score': 0.072625532746315,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical model.'},\n",
       " {'score': 0.049304164946079254,\n",
       "  'token': 5,\n",
       "  'token_str': ' the',\n",
       "  'sequence': 'This course will teach you all about the model.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('This course will teach you all about <mask> model.', top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52cfb7",
   "metadata": {},
   "source": [
    "##### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42bd661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51882b1cddbc414c92e8a4dbf03bd92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cbaa5eed894ffb812d0c087692c316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02afa641724f42d69d11a6b5822b9a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622821c90e4e437299d78c6415400b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd695e9eafda4806bce822a216e8f170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935c313f21f7426dae9df5105c8f933f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model='dslim/bert-base-NER')\n",
    "# grouped_entity=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584686f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': np.float32(0.9928189),\n",
       "  'index': 4,\n",
       "  'word': 'Rabbit',\n",
       "  'start': 11,\n",
       "  'end': 17},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': np.float32(0.99448276),\n",
       "  'index': 9,\n",
       "  'word': 'Play',\n",
       "  'start': 32,\n",
       "  'end': 36},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9833211),\n",
       "  'index': 10,\n",
       "  'word': '##data',\n",
       "  'start': 36,\n",
       "  'end': 40},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.99951816),\n",
       "  'index': 12,\n",
       "  'word': 'Seoul',\n",
       "  'start': 44,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('My name is Rabbit and I work at Playdata in Seoul')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a86c3",
   "metadata": {},
   "source": [
    "##### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "966a66e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93048739f7c248f3b76ac1d3a33f4aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb0bd247c054d438f9eeca3769d9280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a2e6fb705143b699520f2b9d3baa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c20c26ea1554f1c824103acc65533ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597726b49acf448cb1cfa57f50f680ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "qna = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eda9c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9349358779199974, 'start': 32, 'end': 40, 'answer': 'Playdata'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna(question=\"Where do I work?\",\n",
    "    context=\"My name is Rabbit and I work at Playdata in Seoul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07d612",
   "metadata": {},
   "source": [
    "- 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7a9b823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926d95d799434cf481df5038f030098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roxie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roxie\\.cache\\huggingface\\hub\\models--klue--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322b2d5c728b4ec192a87dc01162bc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bb522f30fe4d248df190cefa63ac9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a951fcbfce44f0aaaf6b8bf13ca36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827f42f093ed4b28992699003ccd4910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a890146940476dae912e126644d9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_qna = pipeline('question-answering', model='klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d53de8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 6.684073014184833e-05,\n",
       " 'start': 292,\n",
       " 'end': 317,\n",
       " 'answer': '살던 경기도 장단군 우근리(現 경기도 파주시)'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_qna(question=\"허준 아들의 이름은?\",\n",
    "       context=\"\"\"허준에게는 외아들 허겸(許謙)이 있었다. 허겸은 문과에 급제하여 부사를 거쳐 이후 파릉군(巴陵君)에 봉작받았다. 이후 19대 숙종 때에는 그의 증손자 허진(許瑱)이 파춘군(巴春君)의 작호를 받았으며, 허진의 아들이자 허준의 고손자인 허육(許堉)은 양흥군(陽興君)의 작호를 받았다. 허육의 아들이자 허준의 5대손인 허선(許銑)은 21대 영조 때에 양원군(陽原君)에 올랐으며, 허선의 아들로 허준의 6대손 허흡(許潝) 역시 영조 때 양은군(陽恩君)에 봉작받았다. 이렇게 누대에 걸쳐 후손들이 조정의 관직을 역임했으며, 선대가 살던 경기도 장단군 우근리(現 경기도 파주시)에 대대로 세거했다. 이후 조선 후기에 허준의 10대손 허도(許堵, 1827~1884)가 황해도 해주시로 이주했으며, 13대 종손 허형욱(許亨旭, 1924~몰년 미상)이 1945년까지 그 곳에서 살았다.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "432e6575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on QuestionAnsweringPipeline in module transformers.pipelines.question_answering object:\n",
      "\n",
      "class QuestionAnsweringPipeline(transformers.pipelines.base.ChunkPipeline)\n",
      " |  QuestionAnsweringPipeline(\n",
      " |      model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')],\n",
      " |      tokenizer: transformers.tokenization_utils.PreTrainedTokenizer,\n",
      " |      modelcard: Optional[transformers.modelcard.ModelCard] = None,\n",
      " |      framework: Optional[str] = None,\n",
      " |      task: str = '',\n",
      " |      **kwargs\n",
      " |  )\n",
      " |\n",
      " |  Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question answering\n",
      " |  examples](../task_summary#question-answering) for more information.\n",
      " |\n",
      " |  Example:\n",
      " |\n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |\n",
      " |  >>> oracle = pipeline(model=\"deepset/roberta-base-squad2\")\n",
      " |  >>> oracle(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n",
      " |  {'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}\n",
      " |  ```\n",
      " |\n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |\n",
      " |  This question answering pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"question-answering\"`.\n",
      " |\n",
      " |  The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
      " |  up-to-date list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=question-answering).\n",
      " |\n",
      " |      Arguments:\n",
      " |          model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |              The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |              [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |          tokenizer ([`PreTrainedTokenizer`]):\n",
      " |              The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |              [`PreTrainedTokenizer`].\n",
      " |          modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |              Model card attributed to the model for this pipeline.\n",
      " |          framework (`str`, *optional*):\n",
      " |              The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |              installed.\n",
      " |\n",
      " |              If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |              both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |              provided.\n",
      " |          task (`str`, defaults to `\"\"`):\n",
      " |              A task-identifier for the pipeline.\n",
      " |          num_workers (`int`, *optional*, defaults to 8):\n",
      " |              When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |              workers to be used.\n",
      " |          batch_size (`int`, *optional*, defaults to 1):\n",
      " |              When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |              the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |              pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |          args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |              Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |          device (`int`, *optional*, defaults to -1):\n",
      " |              Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |              the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |          dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |              (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |          binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |              Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |              the raw output data e.g. text.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      QuestionAnsweringPipeline\n",
      " |      transformers.pipelines.base.ChunkPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Answer the question(s) given as inputs by using the context(s).\n",
      " |\n",
      " |      Args:\n",
      " |          question (`str` or `list[str]`):\n",
      " |              One or several question(s) (must be used in conjunction with the `context` argument).\n",
      " |          context (`str` or `list[str]`):\n",
      " |              One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
      " |              `question` argument).\n",
      " |          top_k (`int`, *optional*, defaults to 1):\n",
      " |              The number of answers to return (will be chosen by order of likelihood). Note that we return less than\n",
      " |              top_k answers if there are not enough options available within the context.\n",
      " |          doc_stride (`int`, *optional*, defaults to 128):\n",
      " |              If the context is too long to fit with the question for the model, it will be split in several chunks\n",
      " |              with some overlap. This argument controls the size of that overlap.\n",
      " |          max_answer_len (`int`, *optional*, defaults to 15):\n",
      " |              The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
      " |          max_seq_len (`int`, *optional*, defaults to 384):\n",
      " |              The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\n",
      " |              model. The context will be split in several chunks (using `doc_stride` as overlap) if needed.\n",
      " |          max_question_len (`int`, *optional*, defaults to 64):\n",
      " |              The maximum length of the question after tokenization. It will be truncated if needed.\n",
      " |          handle_impossible_answer (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not we accept impossible as an answer.\n",
      " |          align_to_words (`bool`, *optional*, defaults to `True`):\n",
      " |              Attempts to align the answer to real words. Improves quality on space separated languages. Might hurt on\n",
      " |              non-space-separated languages (like Japanese or Chinese)\n",
      " |\n",
      " |      Return:\n",
      " |          A `dict` or a list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |\n",
      " |          - **score** (`float`) -- The probability associated to the answer.\n",
      " |          - **start** (`int`) -- The character start index of the answer (in the tokenized version of the input).\n",
      " |          - **end** (`int`) -- The character end index of the answer (in the tokenized version of the input).\n",
      " |          - **answer** (`str`) -- The answer to the question.\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')],\n",
      " |      tokenizer: transformers.tokenization_utils.PreTrainedTokenizer,\n",
      " |      modelcard: Optional[transformers.modelcard.ModelCard] = None,\n",
      " |      framework: Optional[str] = None,\n",
      " |      task: str = '',\n",
      " |      **kwargs\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  get_answer(self, answers: list[dict], target: str) -> Optional[dict]\n",
      " |\n",
      " |  get_indices(\n",
      " |      self,\n",
      " |      enc: 'tokenizers.Encoding',\n",
      " |      s: int,\n",
      " |      e: int,\n",
      " |      sequence_index: int,\n",
      " |      align_to_words: bool\n",
      " |  ) -> tuple[int, int]\n",
      " |\n",
      " |  postprocess(\n",
      " |      self,\n",
      " |      model_outputs,\n",
      " |      top_k=1,\n",
      " |      handle_impossible_answer=False,\n",
      " |      max_answer_len=15,\n",
      " |      align_to_words=True\n",
      " |  )\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |\n",
      " |  preprocess(\n",
      " |      self,\n",
      " |      example,\n",
      " |      padding='do_not_pad',\n",
      " |      doc_stride=None,\n",
      " |      max_question_len=64,\n",
      " |      max_seq_len=None\n",
      " |  )\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |\n",
      " |  span_to_answer(self, text: str, start: int, end: int) -> dict[str, typing.Union[str, int]]\n",
      " |      When decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n",
      " |\n",
      " |      Args:\n",
      " |          text (`str`): The actual context to extract the answer from.\n",
      " |          start (`int`): The answer starting token index.\n",
      " |          end (`int`): The answer end token index.\n",
      " |\n",
      " |      Returns:\n",
      " |          Dictionary like `{'answer': str, 'start': int, 'end': int}`\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  create_sample(question: Union[str, list[str]], context: Union[str, list[str]]) -> Union[transformers.data.processors.squad.SquadExample, list[transformers.data.processors.squad.SquadExample]]\n",
      " |      QuestionAnsweringPipeline leverages the [`SquadExample`] internally. This helper method encapsulate all the\n",
      " |      logic for converting question(s) and context(s) to [`SquadExample`].\n",
      " |\n",
      " |      We currently support extractive question answering.\n",
      " |\n",
      " |      Arguments:\n",
      " |          question (`str` or `list[str]`): The question(s) asked.\n",
      " |          context (`str` or `list[str]`): The context(s) in which we will look for the answer.\n",
      " |\n",
      " |      Returns:\n",
      " |          One or a list of [`SquadExample`]: The corresponding [`SquadExample`] grouping question and context.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  default_input_names = 'question,context'\n",
      " |\n",
      " |  handle_impossible_answer = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.ChunkPipeline:\n",
      " |\n",
      " |  get_iterator(\n",
      " |      self,\n",
      " |      inputs,\n",
      " |      num_workers: int,\n",
      " |      batch_size: int,\n",
      " |      preprocess_params,\n",
      " |      forward_params,\n",
      " |      postprocess_params\n",
      " |  )\n",
      " |\n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  check_model_type(self, supported_models: Union[list[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |\n",
      " |      Args:\n",
      " |          supported_models (`list[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |\n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |\n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |\n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |\n",
      " |      Return:\n",
      " |          `dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |\n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |\n",
      " |  get_inference_context(self)\n",
      " |\n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  push_to_hub(\n",
      " |      self,\n",
      " |      repo_id: str,\n",
      " |      use_temp_dir: Optional[bool] = None,\n",
      " |      commit_message: Optional[str] = None,\n",
      " |      private: Optional[bool] = None,\n",
      " |      token: Union[bool, str, NoneType] = None,\n",
      " |      max_shard_size: Union[str, int, NoneType] = '5GB',\n",
      " |      create_pr: bool = False,\n",
      " |      safe_serialization: bool = True,\n",
      " |      revision: Optional[str] = None,\n",
      " |      commit_description: Optional[str] = None,\n",
      " |      tags: Optional[list[str]] = None,\n",
      " |      **deprecated_kwargs\n",
      " |  ) -> str from transformers.utils.hub.PushToHubMixin\n",
      " |      Upload the pipeline file to the 🤗 Model Hub.\n",
      " |\n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`list[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      from transformers import pipeline\n",
      " |\n",
      " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
      " |\n",
      " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
      " |\n",
      " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |\n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  save_pretrained(\n",
      " |      self,\n",
      " |      save_directory: Union[str, os.PathLike],\n",
      " |      safe_serialization: bool = True,\n",
      " |      **kwargs: Any\n",
      " |  )\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |\n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |          kwargs (`dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  dtype\n",
      " |      Dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  torch_dtype\n",
      " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ko_qna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
