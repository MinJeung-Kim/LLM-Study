{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f159a4d2",
   "metadata": {},
   "source": [
    "## NNLM (Neural Network Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8eba5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e20243",
   "metadata": {},
   "source": [
    "텍스트 데이터를 다룰 수 있는 언어모델 `NNLM`을 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c552ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNLM : 단어를 임베딩하고 비선형 계층을 통해 다음 단어의 확률을 계산\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, context_size, hidden_size):\n",
    "        super(NNLM, self).__init__() # nn.Module 초기화\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 단어 임베딩 레이어 \n",
    "        self.fc1 = nn.Linear(embed_size * context_size, hidden_size) # 첫 번째 완전 연결 레이어\n",
    "        self.relu = nn.ReLU() # ReLU 활성화 함수\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size) # 두 번째 완전 연결 레이어\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1) # 로그 소프트맥스 함수\n",
    "\n",
    "    def forward(self, x): # 순전파 함수\n",
    "        embeds = self.embed(x).view((x.size(0), -1)) # 임베딩 후 평탄화\n",
    "        output = self.fc1(embeds) # 첫 번째 완전 연결 레이어 통과\n",
    "        output = self.relu(output) # ReLU 활성화 함수 적용\n",
    "        output = self.fc2(output) # 두 번째 완전 연결 레이어 통과\n",
    "        log_probs = self.log_softmax(output) # 로그 소프트맥스 적용\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee12d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNLM(\n",
       "  (embed): Embedding(5000, 300)\n",
       "  (fc1): Linear(in_features=1200, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=5000, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 5000  # 어휘 사전 크기\n",
    "EMBED_SIZE = 300   # 임베딩 차원\n",
    "CONTEXT_SIZE = 4   # 문맥 크기 (이전 단어 수)\n",
    "HIDDEN_SIZE = 128  # 은닉층 크기\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = NNLM(VOCAB_SIZE, EMBED_SIZE, CONTEXT_SIZE, HIDDEN_SIZE) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80dfbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 샘플 입력 데이터 생성 #############\n",
    "\n",
    "# 0부터 VOCAB_SIZE-1 사이의 정수로 채워진 텐서 생성, 크기 (8, CONTEXT_SIZE)\n",
    "X = torch.randint(0, VOCAB_SIZE, (8, CONTEXT_SIZE))  \n",
    "\n",
    "# 0부터 VOCAB_SIZE-1 사이의 정수로 채워진 텐서 생성, 크기 (8,)\n",
    "y = torch.randint(0, VOCAB_SIZE, (8,))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfa8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.5580\n"
     ]
    }
   ],
   "source": [
    "############# 모델 학습 #############\n",
    "\n",
    "# NLLLoss 손실 함수 사용\n",
    "criterion = nn.NLLLoss() # 음의 로그 우도 손실 함수\n",
    "\n",
    "# optim.Adam 옵티마이저 사용, 학습률 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad() # 기울기 초기화\n",
    "output = model(X) # 순전파\n",
    "loss = criterion(output, y) # 손실 계산\n",
    "loss.backward() # 역전파\n",
    "optimizer.step() # 가중치 업데이트\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fda216",
   "metadata": {},
   "source": [
    "[ 결론 ]  \n",
    "- n-그램 : 확률 기반으로 다음단어를 예측\n",
    "- n-그램 모델을 사용했던걸 신경망 모델로 옮기면서 일반화된 연산을 할 수 있게끔 만들어준거라서,  \n",
    "어떤 형태와 구조를 가지고 있는지 알아두기 위한 실습."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
